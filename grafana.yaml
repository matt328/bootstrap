apiVersion: v1
kind: Namespace
metadata:
  name: monitoring
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  annotations:
    controller-gen.kubebuilder.io/version: v0.9.2
  creationTimestamp: null
  name: podlogs.monitoring.grafana.com
spec:
  group: monitoring.grafana.com
  names:
    categories:
    - grafana-agent
    kind: PodLogs
    listKind: PodLogsList
    plural: podlogs
    singular: podlogs
  scope: Namespaced
  versions:
  - name: v1alpha2
    schema:
      openAPIV3Schema:
        description: PodLogs defines how to collect logs for a Pod.
        properties:
          apiVersion:
            description: 'APIVersion defines the versioned schema of this representation
              of an object. Servers should convert recognized schemas to the latest
              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'
            type: string
          kind:
            description: 'Kind is a string value representing the REST resource this
              object represents. Servers may infer this from the endpoint the client
              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'
            type: string
          metadata:
            type: object
          spec:
            description: PodLogsSpec defines how to collect logs for a Pod.
            properties:
              namespaceSelector:
                description: Selector to select which namespaces the Pod objects are
                  discovered from.
                properties:
                  matchExpressions:
                    description: matchExpressions is a list of label selector requirements.
                      The requirements are ANDed.
                    items:
                      description: A label selector requirement is a selector that
                        contains values, a key, and an operator that relates the key
                        and values.
                      properties:
                        key:
                          description: key is the label key that the selector applies
                            to.
                          type: string
                        operator:
                          description: operator represents a key's relationship to
                            a set of values. Valid operators are In, NotIn, Exists
                            and DoesNotExist.
                          type: string
                        values:
                          description: values is an array of string values. If the
                            operator is In or NotIn, the values array must be non-empty.
                            If the operator is Exists or DoesNotExist, the values
                            array must be empty. This array is replaced during a strategic
                            merge patch.
                          items:
                            type: string
                          type: array
                      required:
                      - key
                      - operator
                      type: object
                    type: array
                  matchLabels:
                    additionalProperties:
                      type: string
                    description: matchLabels is a map of {key,value} pairs. A single
                      {key,value} in the matchLabels map is equivalent to an element
                      of matchExpressions, whose key field is "key", the operator
                      is "In", and the values array contains only "value". The requirements
                      are ANDed.
                    type: object
                type: object
                x-kubernetes-map-type: atomic
              relabelings:
                description: RelabelConfigs to apply to logs before delivering.
                items:
                  description: 'RelabelConfig allows dynamic rewriting of the label
                    set, being applied to samples before ingestion. It defines `<metric_relabel_configs>`-section
                    of Prometheus configuration. More info: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#metric_relabel_configs'
                  properties:
                    action:
                      default: replace
                      description: Action to perform based on regex matching. Default
                        is 'replace'. uppercase and lowercase actions require Prometheus
                        >= 2.36.
                      enum:
                      - replace
                      - Replace
                      - keep
                      - Keep
                      - drop
                      - Drop
                      - hashmod
                      - HashMod
                      - labelmap
                      - LabelMap
                      - labeldrop
                      - LabelDrop
                      - labelkeep
                      - LabelKeep
                      - lowercase
                      - Lowercase
                      - uppercase
                      - Uppercase
                      type: string
                    modulus:
                      description: Modulus to take of the hash of the source label
                        values.
                      format: int64
                      type: integer
                    regex:
                      description: Regular expression against which the extracted
                        value is matched. Default is '(.*)'
                      type: string
                    replacement:
                      description: Replacement value against which a regex replace
                        is performed if the regular expression matches. Regex capture
                        groups are available. Default is '$1'
                      type: string
                    separator:
                      description: Separator placed between concatenated source label
                        values. default is ';'.
                      type: string
                    sourceLabels:
                      description: The source labels select values from existing labels.
                        Their content is concatenated using the configured separator
                        and matched against the configured regular expression for
                        the replace, keep, and drop actions.
                      items:
                        description: LabelName is a valid Prometheus label name which
                          may only contain ASCII letters, numbers, as well as underscores.
                        pattern: ^[a-zA-Z_][a-zA-Z0-9_]*$
                        type: string
                      type: array
                    targetLabel:
                      description: Label to which the resulting value is written in
                        a replace action. It is mandatory for replace actions. Regex
                        capture groups are available.
                      type: string
                  type: object
                type: array
              selector:
                description: Selector to select Pod objects. Required.
                properties:
                  matchExpressions:
                    description: matchExpressions is a list of label selector requirements.
                      The requirements are ANDed.
                    items:
                      description: A label selector requirement is a selector that
                        contains values, a key, and an operator that relates the key
                        and values.
                      properties:
                        key:
                          description: key is the label key that the selector applies
                            to.
                          type: string
                        operator:
                          description: operator represents a key's relationship to
                            a set of values. Valid operators are In, NotIn, Exists
                            and DoesNotExist.
                          type: string
                        values:
                          description: values is an array of string values. If the
                            operator is In or NotIn, the values array must be non-empty.
                            If the operator is Exists or DoesNotExist, the values
                            array must be empty. This array is replaced during a strategic
                            merge patch.
                          items:
                            type: string
                          type: array
                      required:
                      - key
                      - operator
                      type: object
                    type: array
                  matchLabels:
                    additionalProperties:
                      type: string
                    description: matchLabels is a map of {key,value} pairs. A single
                      {key,value} in the matchLabels map is equivalent to an element
                      of matchExpressions, whose key field is "key", the operator
                      is "In", and the values array contains only "value". The requirements
                      are ANDed.
                    type: object
                type: object
                x-kubernetes-map-type: atomic
            required:
            - selector
            type: object
        type: object
    served: true
    storage: true
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: grafana
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana
    app.kubernetes.io/version: 9.5.1
    helm.sh/chart: grafana-6.56.2
  name: grafana
  namespace: monitoring
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: grafana-agent
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana-agent
    app.kubernetes.io/version: v0.33.1
    helm.sh/chart: grafana-agent-0.13.0
  name: grafana-agent
  namespace: monitoring
---
apiVersion: v1
imagePullSecrets: null
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: metrics
    app.kubernetes.io/instance: kube-state-metrics
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/version: 2.8.2
    helm.sh/chart: kube-state-metrics-5.6.1
  name: kube-state-metrics
  namespace: monitoring
---
apiVersion: v1
automountServiceAccountToken: true
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: loki
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: loki
    app.kubernetes.io/version: 2.8.2
    helm.sh/chart: loki-5.5.0
  name: loki
  namespace: monitoring
---
apiVersion: v1
automountServiceAccountToken: true
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: canary
    app.kubernetes.io/instance: loki
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: loki
    app.kubernetes.io/version: 2.8.2
    helm.sh/chart: loki-5.5.0
  name: loki-canary
  namespace: monitoring
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/version: 2.8.0
    helm.sh/chart: mimir-distributed-4.4.1
  name: mimir
  namespace: monitoring
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: rollout-operator
    app.kubernetes.io/version: v0.4.0
    helm.sh/chart: rollout-operator-0.4.2
  name: mimir-rollout-operator
  namespace: monitoring
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: minio-sa
  namespace: monitoring
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/instance: grafana
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana
    app.kubernetes.io/version: 9.5.1
    helm.sh/chart: grafana-6.56.2
  name: grafana
  namespace: monitoring
rules: []
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: mimir-rollout-operator
  namespace: monitoring
rules:
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - list
  - get
  - watch
  - delete
- apiGroups:
  - apps
  resources:
  - statefulsets
  verbs:
  - list
  - get
  - watch
- apiGroups:
  - apps
  resources:
  - statefulsets/status
  verbs:
  - update
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: grafana-agent
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana-agent
    app.kubernetes.io/version: v0.33.1
    helm.sh/chart: grafana-agent-0.13.0
  name: grafana-agent
rules:
- apiGroups:
  - ""
  - discovery.k8s.io
  - networking.k8s.io
  resources:
  - endpoints
  - endpointslices
  - ingresses
  - nodes
  - nodes/proxy
  - nodes/metrics
  - pods
  - services
  - nodes/metrics
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - pods
  - pods/log
  - namespaces
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - monitoring.grafana.com
  resources:
  - podlogs
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - monitoring.coreos.com
  resources:
  - prometheusrules
  verbs:
  - get
  - list
  - watch
- nonResourceURLs:
  - /metrics
  verbs:
  - get
- apiGroups:
  - monitoring.coreos.com
  resources:
  - podmonitors
  - servicemonitors
  - probes
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - get
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: grafana
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana
    app.kubernetes.io/version: 9.5.1
    helm.sh/chart: grafana-6.56.2
  name: grafana-clusterrole
rules: []
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/component: metrics
    app.kubernetes.io/instance: kube-state-metrics
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/version: 2.8.2
    helm.sh/chart: kube-state-metrics-5.6.1
  name: kube-state-metrics
rules:
- apiGroups:
  - certificates.k8s.io
  resources:
  - certificatesigningrequests
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - configmaps
  verbs:
  - list
  - watch
- apiGroups:
  - batch
  resources:
  - cronjobs
  verbs:
  - list
  - watch
- apiGroups:
  - extensions
  - apps
  resources:
  - daemonsets
  verbs:
  - list
  - watch
- apiGroups:
  - extensions
  - apps
  resources:
  - deployments
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - endpoints
  verbs:
  - list
  - watch
- apiGroups:
  - autoscaling
  resources:
  - horizontalpodautoscalers
  verbs:
  - list
  - watch
- apiGroups:
  - extensions
  - networking.k8s.io
  resources:
  - ingresses
  verbs:
  - list
  - watch
- apiGroups:
  - batch
  resources:
  - jobs
  verbs:
  - list
  - watch
- apiGroups:
  - coordination.k8s.io
  resources:
  - leases
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - limitranges
  verbs:
  - list
  - watch
- apiGroups:
  - admissionregistration.k8s.io
  resources:
  - mutatingwebhookconfigurations
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - namespaces
  verbs:
  - list
  - watch
- apiGroups:
  - networking.k8s.io
  resources:
  - networkpolicies
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - persistentvolumeclaims
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - persistentvolumes
  verbs:
  - list
  - watch
- apiGroups:
  - policy
  resources:
  - poddisruptionbudgets
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - list
  - watch
- apiGroups:
  - extensions
  - apps
  resources:
  - replicasets
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - replicationcontrollers
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - resourcequotas
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - secrets
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - services
  verbs:
  - list
  - watch
- apiGroups:
  - apps
  resources:
  - statefulsets
  verbs:
  - list
  - watch
- apiGroups:
  - storage.k8s.io
  resources:
  - storageclasses
  verbs:
  - list
  - watch
- apiGroups:
  - admissionregistration.k8s.io
  resources:
  - validatingwebhookconfigurations
  verbs:
  - list
  - watch
- apiGroups:
  - storage.k8s.io
  resources:
  - volumeattachments
  verbs:
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: grafana
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana
    app.kubernetes.io/version: 9.5.1
    helm.sh/chart: grafana-6.56.2
  name: grafana
  namespace: monitoring
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: grafana
subjects:
- kind: ServiceAccount
  name: grafana
  namespace: monitoring
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: mimir-rollout-operator
  namespace: monitoring
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: mimir-rollout-operator
subjects:
- kind: ServiceAccount
  name: mimir-rollout-operator
  namespace: monitoring
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: grafana-agent
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana-agent
    app.kubernetes.io/version: v0.33.1
    helm.sh/chart: grafana-agent-0.13.0
  name: grafana-agent
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: grafana-agent
subjects:
- kind: ServiceAccount
  name: grafana-agent
  namespace: monitoring
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: grafana
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana
    app.kubernetes.io/version: 9.5.1
    helm.sh/chart: grafana-6.56.2
  name: grafana-clusterrolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: grafana-clusterrole
subjects:
- kind: ServiceAccount
  name: grafana
  namespace: monitoring
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/component: metrics
    app.kubernetes.io/instance: kube-state-metrics
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/version: 2.8.2
    helm.sh/chart: kube-state-metrics-5.6.1
  name: kube-state-metrics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kube-state-metrics
subjects:
- kind: ServiceAccount
  name: kube-state-metrics
  namespace: monitoring
---
apiVersion: v1
data:
  dashboardproviders.yaml: |
    apiVersion: 1
    providers:
    - disableDeletion: true
      editable: true
      folder: Kubernetes
      name: grafana-dashboards-kubernetes
      options:
        path: /var/lib/grafana/dashboards/grafana-dashboards-kubernetes
      orgId: 1
      type: file
  datasources.yaml: |
    apiVersion: 1
    datasources:
    - name: Mimir
      type: prometheus
      url: http://mimir-nginx/prometheus
    - name: Loki
      type: loki
      url: http://loki-gateway
  download_dashboards.sh: "#!/usr/bin/env sh\nset -euf\nmkdir -p /var/lib/grafana/dashboards/grafana-dashboards-kubernetes\n\ncurl
    -skf \\\n--connect-timeout 60 \\\n--max-time 60 \\\n-H \"Accept: application/json\"
    \\\n-H \"Content-Type: application/json;charset=UTF-8\" \\\n  \"https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-system-api-server.json\"
    \\\n> \"/var/lib/grafana/dashboards/grafana-dashboards-kubernetes/k8s-system-api-server.json\"\n
    \ \ncurl -skf \\\n--connect-timeout 60 \\\n--max-time 60 \\\n-H \"Accept: application/json\"
    \\\n-H \"Content-Type: application/json;charset=UTF-8\" \\\n  \"https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-system-coredns.json\"
    \\\n> \"/var/lib/grafana/dashboards/grafana-dashboards-kubernetes/k8s-system-coredns.json\"\n
    \ \ncurl -skf \\\n--connect-timeout 60 \\\n--max-time 60 \\\n-H \"Accept: application/json\"
    \\\n-H \"Content-Type: application/json;charset=UTF-8\" \\\n  \"https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-global.json\"
    \\\n> \"/var/lib/grafana/dashboards/grafana-dashboards-kubernetes/k8s-views-global.json\"\n
    \ \ncurl -skf \\\n--connect-timeout 60 \\\n--max-time 60 \\\n-H \"Accept: application/json\"
    \\\n-H \"Content-Type: application/json;charset=UTF-8\" \\\n  \"https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-namespaces.json\"
    \\\n> \"/var/lib/grafana/dashboards/grafana-dashboards-kubernetes/k8s-views-namespaces.json\"\n
    \ \ncurl -skf \\\n--connect-timeout 60 \\\n--max-time 60 \\\n-H \"Accept: application/json\"
    \\\n-H \"Content-Type: application/json;charset=UTF-8\" \\\n  \"https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-nodes.json\"
    \\\n> \"/var/lib/grafana/dashboards/grafana-dashboards-kubernetes/k8s-views-nodes.json\"\n
    \ \ncurl -skf \\\n--connect-timeout 60 \\\n--max-time 60 \\\n-H \"Accept: application/json\"
    \\\n-H \"Content-Type: application/json;charset=UTF-8\" \\\n  \"https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-pods.json\"
    \\\n> \"/var/lib/grafana/dashboards/grafana-dashboards-kubernetes/k8s-views-pods.json\"\n"
  grafana.ini: |
    [analytics]
    check_for_updates = true
    [grafana_net]
    url = https://grafana.net
    [log]
    mode = console
    [paths]
    data = /var/lib/grafana/
    logs = /var/log/grafana
    plugins = /var/lib/grafana/plugins
    provisioning = /etc/grafana/provisioning
    [server]
    domain = grafana.k8s.local
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: grafana
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana
    app.kubernetes.io/version: 9.5.1
    helm.sh/chart: grafana-6.56.2
  name: grafana
  namespace: monitoring
---
apiVersion: v1
data:
  config.river: "// Logs\r\n\r\ndiscovery.kubernetes \"all_pods\" {\r\n  role = \"pod\"\r\n}\r\n\r\ndiscovery.relabel
    \"all_pods\" {\r\n  targets = discovery.kubernetes.all_pods.targets\r\n  rule
    {\r\n    source_labels = [\"__meta_kubernetes_namespace\"]\r\n    target_label
    = \"namespace\"\r\n  }\r\n  rule {\r\n    source_labels = [\"__meta_kubernetes_pod_name\"]\r\n
    \   target_label = \"pod\"\r\n  }\r\n  rule {\r\n    source_labels = [\"__meta_kubernetes_pod_container_name\"]\r\n
    \   target_label = \"container\"\r\n  }\r\n  rule {\r\n    source_labels = [\"__meta_kubernetes_pod_controller_name\"]\r\n
    \   target_label = \"controller\"\r\n  }\r\n}\r\n\r\nloki.source.kubernetes \"all_pods\"
    {\r\n  targets = discovery.relabel.all_pods.output\r\n  forward_to = [loki.process.all_pods.receiver]\r\n}\r\n\r\nloki.process
    \"all_pods\" {\r\n  forward_to = [loki.write.loki.receiver]\r\n\r\n  stage.cri
    {}\r\n}\r\n\r\nloki.source.journal \"read\" {\r\n  forward_to = [loki.relabel.journal.receiver]\r\n}\r\n\r\nloki.relabel
    \"journal\" {\r\n  forward_to = [loki.write.loki.receiver]\r\n  rule {\r\n    source_labels
    = [\"__journal__systemd_unit\"]\r\n    target_label = \"unit\"\r\n  }\r\n}\r\n\r\nloki.source.kubernetes_events
    \"events\" {\r\n  forward_to = [loki.write.loki.receiver]\r\n}\r\n\r\nloki.write
    \"loki\" {\r\n  external_labels = {\r\n    cluster = \"loutretel\",\r\n  }\r\n
    \ endpoint {\r\n    url = \"http://loki-gateway/loki/api/v1/push\"\r\n  }\r\n}\r\n\r\n\r\n//
    Metrics\r\n\r\ndiscovery.kubernetes \"all_nodes\" {\r\n  role = \"node\"\r\n}\r\n\r\ndiscovery.relabel
    \"all_nodes\" {\r\n  targets = discovery.kubernetes.all_nodes.targets\r\n  rule
    {\r\n    source_labels = [\"__meta_kubernetes_node_name\"]\r\n    target_label
    = \"node\"\r\n  }\r\n}\r\n\r\nprometheus.scrape \"cadvisor\" {\r\n  targets =
    discovery.relabel.all_nodes.output\r\n  forward_to = [prometheus.remote_write.mimir.receiver]\r\n\r\n
    \ scrape_interval = \"15s\"\r\n  metrics_path = \"/metrics/cadvisor\"\r\n  scheme
    = \"https\"\r\n\r\n  bearer_token_file = \"/var/run/secrets/kubernetes.io/serviceaccount/token\"\r\n
    \ tls_config {\r\n    ca_file = \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\"\r\n
    \ }\r\n}\r\n\r\nprometheus.scrape \"kubelet\" {\r\n  targets = discovery.relabel.all_nodes.output\r\n
    \ forward_to = [prometheus.remote_write.mimir.receiver]\r\n\r\n  scrape_interval
    = \"15s\"\r\n  metrics_path = \"/metrics\"\r\n  scheme = \"https\"\r\n\r\n  bearer_token_file
    = \"/var/run/secrets/kubernetes.io/serviceaccount/token\"\r\n  tls_config {\r\n
    \   ca_file = \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\"\r\n  }\r\n}\r\n\r\nprometheus.exporter.unix
    {\r\n  procfs_path = \"/host/proc\"\r\n  sysfs_path = \"/host/sys\"\r\n  rootfs_path
    = \"/host/root\"\r\n}\r\n\r\n\r\nprometheus.scrape \"node_exporter\" {\r\n  targets
    = prometheus.exporter.unix.targets\r\n  forward_to = [prometheus.relabel.node_exporter.receiver]\r\n\r\n
    \ job_name = \"node-exporter\"\r\n  scrape_interval = \"15s\"\r\n}\r\n\r\nprometheus.relabel
    \"node_exporter\" {\r\n  forward_to = [prometheus.remote_write.mimir.receiver]\r\n\r\n
    \ rule {\r\n    replacement = env(\"HOSTNAME\")\r\n    target_label = \"nodename\"\r\n
    \ }\r\n\r\n  rule {\r\n    replacement = \"node-exporter\"\r\n    target_label
    = \"job\"\r\n  }\r\n}\r\n\r\n\r\ndiscovery.kubernetes \"all_services\" {\r\n  role
    = \"service\"\r\n}\r\n\r\ndiscovery.relabel \"kube_state_metrics\" {\r\n  targets
    = discovery.kubernetes.all_services.targets\r\n\r\n  rule {\r\n    action = \"keep\"\r\n
    \   source_labels = [\"__meta_kubernetes_service_name\"]\r\n    regex = \"kube-state-metrics\"\r\n
    \ }\r\n}\r\n\r\nprometheus.scrape \"kube_state_metrics\" {\r\n  targets = discovery.relabel.kube_state_metrics.output\r\n
    \ forward_to = [prometheus.remote_write.mimir.receiver]\r\n\r\n  scrape_interval
    = \"15s\"\r\n}\r\n\r\n\r\nprometheus.remote_write \"mimir\" {\r\n  external_labels
    = {\r\n    cluster = \"loutretel\",\r\n  }\r\n  endpoint {\r\n    url = \"http://mimir-nginx/api/v1/push\"\r\n
    \ }\r\n}\r\n"
kind: ConfigMap
metadata:
  name: grafana-agent
  namespace: monitoring
---
apiVersion: v1
data: {}
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: grafana
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana
    app.kubernetes.io/version: 9.5.1
    dashboard-provider: grafana-dashboards-kubernetes
    helm.sh/chart: grafana-6.56.2
  name: grafana-dashboards-grafana-dashboards-kubernetes
  namespace: monitoring
---
apiVersion: v1
data:
  config.yaml: |
    auth_enabled: false
    common:
      compactor_address: 'loki'
      path_prefix: /var/loki
      replication_factor: 1
      storage:
        filesystem:
          chunks_directory: /var/loki/chunks
          rules_directory: /var/loki/rules
    index_gateway:
      mode: ring
    limits_config:
      enforce_metric_name: false
      max_cache_freshness_per_query: 10m
      reject_old_samples: true
      reject_old_samples_max_age: 168h
      split_queries_by_interval: 15m
    memberlist:
      join_members:
      - loki-memberlist
    query_range:
      align_queries_with_step: true
    ruler:
      storage:
        type: local
    runtime_config:
      file: /etc/loki/runtime-config/runtime-config.yaml
    schema_config:
      configs:
      - from: "2022-01-11"
        index:
          period: 24h
          prefix: loki_index_
        object_store: filesystem
        schema: v12
        store: boltdb-shipper
    server:
      grpc_listen_port: 9095
      http_listen_port: 3100
    storage_config:
      hedging:
        at: 250ms
        max_per_second: 20
        up_to: 3
    table_manager:
      retention_deletes_enabled: false
      retention_period: 0
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: loki
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: loki
    app.kubernetes.io/version: 2.8.2
    helm.sh/chart: loki-5.5.0
  name: loki
  namespace: monitoring
---
apiVersion: v1
data:
  nginx.conf: "worker_processes  5;  ## Default: 1\nerror_log  /dev/stderr;\npid        /tmp/nginx.pid;\nworker_rlimit_nofile
    8192;\n\nevents {\n  worker_connections  4096;  ## Default: 1024\n}\n\nhttp {\n
    \ client_body_temp_path /tmp/client_temp;\n  proxy_temp_path       /tmp/proxy_temp_path;\n
    \ fastcgi_temp_path     /tmp/fastcgi_temp;\n  uwsgi_temp_path       /tmp/uwsgi_temp;\n
    \ scgi_temp_path        /tmp/scgi_temp;\n\n  client_max_body_size 4M;\n\n  proxy_read_timeout
    \   600; ## 6 minutes\n  proxy_send_timeout    600;\n  proxy_connect_timeout 600;\n\n
    \ proxy_http_version    1.1;\n\n  default_type application/octet-stream;\n  log_format
    \  main '$remote_addr - $remote_user [$time_local]  $status '\n        '\"$request\"
    $body_bytes_sent \"$http_referer\" '\n        '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n
    \ access_log   /dev/stderr  main;\n\n  sendfile     on;\n  tcp_nopush   on;\n
    \ resolver kube-dns.kube-system.svc.cluster.local.;\n  \n\n  server {\n    listen
    \            8080;\n\n    location = / {\n      return 200 'OK';\n      auth_basic
    off;\n    }\n\n\n    # Distributor\n    location = /api/prom/push {\n      proxy_pass
    \      http://loki.monitoring.svc.cluster.local:3100$request_uri;\n    }\n    location
    = /loki/api/v1/push {\n      proxy_pass       http://loki.monitoring.svc.cluster.local:3100$request_uri;\n
    \   }\n    location = /distributor/ring {\n      proxy_pass       http://loki.monitoring.svc.cluster.local:3100$request_uri;\n
    \   }\n\n    # Ingester\n    location = /flush {\n      proxy_pass       http://loki.monitoring.svc.cluster.local:3100$request_uri;\n
    \   }\n    location ^~ /ingester/ {\n      proxy_pass       http://loki.monitoring.svc.cluster.local:3100$request_uri;\n
    \   }\n    location = /ingester {\n      internal;        # to suppress 301\n
    \   }\n\n    # Ring\n    location = /ring {\n      proxy_pass       http://loki.monitoring.svc.cluster.local:3100$request_uri;\n
    \   }\n\n    # MemberListKV\n    location = /memberlist {\n      proxy_pass       http://loki.monitoring.svc.cluster.local:3100$request_uri;\n
    \   }\n\n\n    # Ruler\n    location = /ruler/ring {\n      proxy_pass       http://loki.monitoring.svc.cluster.local:3100$request_uri;\n
    \   }\n    location = /api/prom/rules {\n      proxy_pass       http://loki.monitoring.svc.cluster.local:3100$request_uri;\n
    \   }\n    location ^~ /api/prom/rules/ {\n      proxy_pass       http://loki.monitoring.svc.cluster.local:3100$request_uri;\n
    \   }\n    location = /loki/api/v1/rules {\n      proxy_pass       http://loki.monitoring.svc.cluster.local:3100$request_uri;\n
    \   }\n    location ^~ /loki/api/v1/rules/ {\n      proxy_pass       http://loki.monitoring.svc.cluster.local:3100$request_uri;\n
    \   }\n    location = /prometheus/api/v1/alerts {\n      proxy_pass       http://loki.monitoring.svc.cluster.local:3100$request_uri;\n
    \   }\n    location = /prometheus/api/v1/rules {\n      proxy_pass       http://loki.monitoring.svc.cluster.local:3100$request_uri;\n
    \   }\n\n    # Compactor\n    location = /compactor/ring {\n      proxy_pass       http://loki.monitoring.svc.cluster.local:3100$request_uri;\n
    \   }\n    location = /loki/api/v1/delete {\n      proxy_pass       http://loki.monitoring.svc.cluster.local:3100$request_uri;\n
    \   }\n    location = /loki/api/v1/cache/generation_numbers {\n      proxy_pass
    \      http://loki.monitoring.svc.cluster.local:3100$request_uri;\n    }\n\n    #
    IndexGateway\n    location = /indexgateway/ring {\n      proxy_pass       http://loki.monitoring.svc.cluster.local:3100$request_uri;\n
    \   }\n\n    # QueryScheduler\n    location = /scheduler/ring {\n      proxy_pass
    \      http://loki.monitoring.svc.cluster.local:3100$request_uri;\n    }\n\n\n
    \   # QueryFrontend, Querier\n    location = /api/prom/tail {\n      proxy_pass
    \      http://loki.monitoring.svc.cluster.local:3100$request_uri;\n      proxy_set_header
    Upgrade $http_upgrade;\n      proxy_set_header Connection \"upgrade\";\n    }\n
    \   location = /loki/api/v1/tail {\n      proxy_pass       http://loki.monitoring.svc.cluster.local:3100$request_uri;\n
    \     proxy_set_header Upgrade $http_upgrade;\n      proxy_set_header Connection
    \"upgrade\";\n    }\n    location ^~ /api/prom/ {\n      proxy_pass       http://loki.monitoring.svc.cluster.local:3100$request_uri;\n
    \   }\n    location = /api/prom {\n      internal;        # to suppress 301\n
    \   }\n    location ^~ /loki/api/v1/ {\n      proxy_pass       http://loki.monitoring.svc.cluster.local:3100$request_uri;\n
    \   }\n    location = /loki/api/v1 {\n      internal;        # to suppress 301\n
    \   }\n  }\n}\n"
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: gateway
    app.kubernetes.io/instance: loki
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: loki
    app.kubernetes.io/version: 2.8.2
    helm.sh/chart: loki-5.5.0
  name: loki-gateway
  namespace: monitoring
---
apiVersion: v1
data:
  runtime-config.yaml: |2

    {}
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: loki
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: loki
    app.kubernetes.io/version: 2.8.2
    helm.sh/chart: loki-5.5.0
  name: loki-runtime
  namespace: monitoring
---
apiVersion: v1
data:
  alertmanager_fallback_config.yaml: |
    receivers:
        - name: default-receiver
    route:
        receiver: default-receiver
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: alertmanager
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: 2.8.0
    helm.sh/chart: mimir-distributed-4.4.1
  name: mimir-alertmanager-fallback-config
  namespace: monitoring
---
apiVersion: v1
data:
  mimir.yaml: |2

    activity_tracker:
      filepath: /active-query-tracker/activity.log
    alertmanager:
      data_dir: /data
      enable_api: true
      external_url: /alertmanager
      fallback_config_file: /configs/alertmanager_fallback_config.yaml
    alertmanager_storage:
      backend: s3
      s3:
        access_key_id: grafana-mimir
        bucket_name: mimir-ruler
        endpoint: mimir-minio.monitoring.svc:9000
        insecure: true
        secret_access_key: supersecret
    blocks_storage:
      backend: s3
      bucket_store:
        max_chunk_pool_bytes: 12884901888
        sync_dir: /data/tsdb-sync
      s3:
        access_key_id: grafana-mimir
        bucket_name: mimir-tsdb
        endpoint: mimir-minio.monitoring.svc:9000
        insecure: true
        secret_access_key: supersecret
      tsdb:
        dir: /data/tsdb
        head_compaction_interval: 15m
        wal_replay_concurrency: 3
    compactor:
      compaction_interval: 30m
      data_dir: /data
      deletion_delay: 2h
      first_level_compaction_wait_period: 25m
      max_closing_blocks_concurrency: 2
      max_opening_blocks_concurrency: 4
      sharding_ring:
        wait_stability_min_duration: 1m
      symbols_flushers_concurrency: 4
    frontend:
      parallelize_shardable_queries: true
      scheduler_address: mimir-query-scheduler-headless.monitoring.svc:9095
    frontend_worker:
      grpc_client_config:
        max_send_msg_size: 419430400
      scheduler_address: mimir-query-scheduler-headless.monitoring.svc:9095
    ingester:
      ring:
        final_sleep: 0s
        num_tokens: 512
        tokens_file_path: /data/tokens
        unregister_on_shutdown: false
    ingester_client:
      grpc_client_config:
        max_recv_msg_size: 104857600
        max_send_msg_size: 104857600
    limits:
      compactor_blocks_retention_period: 30d
      max_cache_freshness: 10m
      max_label_names_per_series: 60
      max_query_parallelism: 240
      max_total_query_length: 12000h
    memberlist:
      abort_if_cluster_join_fails: false
      compression_enabled: false
      join_members:
      - dns+mimir-gossip-ring.monitoring.svc.cluster.local:7946
    multitenancy_enabled: false
    querier:
      max_concurrent: 16
    query_scheduler:
      max_outstanding_requests_per_tenant: 800
    ruler:
      alertmanager_url: dnssrvnoa+http://_http-metrics._tcp.mimir-alertmanager-headless.monitoring.svc.cluster.local/alertmanager
      enable_api: true
      rule_path: /data
    ruler_storage:
      backend: s3
      s3:
        access_key_id: grafana-mimir
        bucket_name: mimir-ruler
        endpoint: mimir-minio.monitoring.svc:9000
        insecure: true
        secret_access_key: supersecret
    runtime_config:
      file: /var/mimir/runtime.yaml
    server:
      grpc_server_max_concurrent_streams: 1000
      grpc_server_max_connection_age: 2m
      grpc_server_max_connection_age_grace: 5m
      grpc_server_max_connection_idle: 1m
    store_gateway:
      sharding_ring:
        tokens_file_path: /data/tokens
        unregister_on_shutdown: false
        wait_stability_min_duration: 1m
    usage_stats:
      installation_mode: helm
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/version: 2.8.0
    helm.sh/chart: mimir-distributed-4.4.1
  name: mimir-config
  namespace: monitoring
---
apiVersion: v1
data:
  add-policy: |-
    #!/bin/sh
    set -e ; # Have script exit in the event of a failed command.
    MC_CONFIG_DIR="/tmp/minio/mc/"
    MC="/usr/bin/mc --insecure --config-dir ${MC_CONFIG_DIR}"

    # connectToMinio
    # Use a check-sleep-check loop to wait for MinIO service to be available
    connectToMinio() {
      SCHEME=$1
      ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts
      set -e ; # fail if we can't read the keys.
      ACCESS=$(cat /config/rootUser) ; SECRET=$(cat /config/rootPassword) ;
      set +e ; # The connections to minio are allowed to fail.
      echo "Connecting to MinIO server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT" ;
      MC_COMMAND="${MC} alias set myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET" ;
      $MC_COMMAND ;
      STATUS=$? ;
      until [ $STATUS = 0 ]
      do
        ATTEMPTS=`expr $ATTEMPTS + 1` ;
        echo \"Failed attempts: $ATTEMPTS\" ;
        if [ $ATTEMPTS -gt $LIMIT ]; then
          exit 1 ;
        fi ;
        sleep 2 ; # 1 second intervals between attempts
        $MC_COMMAND ;
        STATUS=$? ;
      done ;
      set -e ; # reset `e` as active
      return 0
    }

    # checkPolicyExists ($policy)
    # Check if the policy exists, by using the exit code of `mc admin policy info`
    checkPolicyExists() {
      POLICY=$1
      CMD=$(${MC} admin policy info myminio $POLICY > /dev/null 2>&1)
      return $?
    }

    # createPolicy($name, $filename)
    createPolicy () {
      NAME=$1
      FILENAME=$2

      # Create the name if it does not exist
      echo "Checking policy: $NAME (in /config/$FILENAME.json)"
      if ! checkPolicyExists $NAME ; then
        echo "Creating policy '$NAME'"
      else
        echo "Policy '$NAME' already exists."
      fi
      ${MC} admin policy add myminio $NAME /config/$FILENAME.json

    }

    # Try connecting to MinIO instance
    scheme=http
    connectToMinio $scheme
  add-svcacct: |-
    #!/bin/sh
    set -e ; # Have script exit in the event of a failed command.
    MC_CONFIG_DIR="/tmp/minio/mc/"
    MC="/usr/bin/mc --insecure --config-dir ${MC_CONFIG_DIR}"

    # AccessKey and secretkey credentials file are added to prevent shell execution errors caused by special characters.
    # Special characters for example : ',",<,>,{,}
    MINIO_ACCESSKEY_SECRETKEY_TMP="/tmp/accessKey_and_secretKey_svcacct_tmp"

    # connectToMinio
    # Use a check-sleep-check loop to wait for MinIO service to be available
    connectToMinio() {
      SCHEME=$1
      ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts
      set -e ; # fail if we can't read the keys.
      ACCESS=$(cat /config/rootUser) ; SECRET=$(cat /config/rootPassword) ;
      set +e ; # The connections to minio are allowed to fail.
      echo "Connecting to MinIO server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT" ;
      MC_COMMAND="${MC} alias set myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET" ;
      $MC_COMMAND ;
      STATUS=$? ;
      until [ $STATUS = 0 ]
      do
        ATTEMPTS=`expr $ATTEMPTS + 1` ;
        echo \"Failed attempts: $ATTEMPTS\" ;
        if [ $ATTEMPTS -gt $LIMIT ]; then
          exit 1 ;
        fi ;
        sleep 2 ; # 2 second intervals between attempts
        $MC_COMMAND ;
        STATUS=$? ;
      done ;
      set -e ; # reset `e` as active
      return 0
    }

    # checkSvcacctExists ()
    # Check if the svcacct exists, by using the exit code of `mc admin user svcacct info`
    checkSvcacctExists() {
      CMD=$(${MC} admin user svcacct info myminio $(head -1 $MINIO_ACCESSKEY_SECRETKEY_TMP) > /dev/null 2>&1)
      return $?
    }

    # createSvcacct ($user)
    createSvcacct () {
      USER=$1
      FILENAME=$2
      #check accessKey_and_secretKey_tmp file
      if [[ ! -f $MINIO_ACCESSKEY_SECRETKEY_TMP ]];then
        echo "credentials file does not exist"
        return 1
      fi
      if [[ $(cat $MINIO_ACCESSKEY_SECRETKEY_TMP|wc -l) -ne 2 ]];then
        echo "credentials file is invalid"
        rm -f $MINIO_ACCESSKEY_SECRETKEY_TMP
        return 1
      fi
      SVCACCT=$(head -1 $MINIO_ACCESSKEY_SECRETKEY_TMP)
      # Create the svcacct if it does not exist
      if ! checkSvcacctExists ; then
        echo "Creating svcacct '$SVCACCT'"
        # Check if policy file is define
        if [ -z $FILENAME ]; then
          ${MC} admin user svcacct add --access-key $(head -1 $MINIO_ACCESSKEY_SECRETKEY_TMP) --secret-key $(tail -n1 $MINIO_ACCESSKEY_SECRETKEY_TMP) myminio $USER
        else
          ${MC} admin user svcacct add --access-key $(head -1 $MINIO_ACCESSKEY_SECRETKEY_TMP) --secret-key $(tail -n1 $MINIO_ACCESSKEY_SECRETKEY_TMP) --policy /config/$FILENAME.json myminio $USER
        fi
      else
        echo "Svcacct '$SVCACCT' already exists."
      fi
      #clean up credentials files.
      rm -f $MINIO_ACCESSKEY_SECRETKEY_TMP
    }

    # Try connecting to MinIO instance
    scheme=http
    connectToMinio $scheme
  add-user: |-
    #!/bin/sh
    set -e ; # Have script exit in the event of a failed command.
    MC_CONFIG_DIR="/tmp/minio/mc/"
    MC="/usr/bin/mc --insecure --config-dir ${MC_CONFIG_DIR}"

    # AccessKey and secretkey credentials file are added to prevent shell execution errors caused by special characters.
    # Special characters for example : ',",<,>,{,}
    MINIO_ACCESSKEY_SECRETKEY_TMP="/tmp/accessKey_and_secretKey_tmp"

    # connectToMinio
    # Use a check-sleep-check loop to wait for MinIO service to be available
    connectToMinio() {
      SCHEME=$1
      ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts
      set -e ; # fail if we can't read the keys.
      ACCESS=$(cat /config/rootUser) ; SECRET=$(cat /config/rootPassword) ;
      set +e ; # The connections to minio are allowed to fail.
      echo "Connecting to MinIO server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT" ;
      MC_COMMAND="${MC} alias set myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET" ;
      $MC_COMMAND ;
      STATUS=$? ;
      until [ $STATUS = 0 ]
      do
        ATTEMPTS=`expr $ATTEMPTS + 1` ;
        echo \"Failed attempts: $ATTEMPTS\" ;
        if [ $ATTEMPTS -gt $LIMIT ]; then
          exit 1 ;
        fi ;
        sleep 2 ; # 1 second intervals between attempts
        $MC_COMMAND ;
        STATUS=$? ;
      done ;
      set -e ; # reset `e` as active
      return 0
    }

    # checkUserExists ()
    # Check if the user exists, by using the exit code of `mc admin user info`
    checkUserExists() {
      CMD=$(${MC} admin user info myminio $(head -1 $MINIO_ACCESSKEY_SECRETKEY_TMP) > /dev/null 2>&1)
      return $?
    }

    # createUser ($policy)
    createUser() {
      POLICY=$1
      #check accessKey_and_secretKey_tmp file
      if [[ ! -f $MINIO_ACCESSKEY_SECRETKEY_TMP ]];then
        echo "credentials file does not exist"
        return 1
      fi
      if [[ $(cat $MINIO_ACCESSKEY_SECRETKEY_TMP|wc -l) -ne 2 ]];then
        echo "credentials file is invalid"
        rm -f $MINIO_ACCESSKEY_SECRETKEY_TMP
        return 1
      fi
      USER=$(head -1 $MINIO_ACCESSKEY_SECRETKEY_TMP)
      # Create the user if it does not exist
      if ! checkUserExists ; then
        echo "Creating user '$USER'"
        cat $MINIO_ACCESSKEY_SECRETKEY_TMP | ${MC} admin user add myminio
      else
        echo "User '$USER' already exists."
      fi
      #clean up credentials files.
      rm -f $MINIO_ACCESSKEY_SECRETKEY_TMP

      # set policy for user
      if [ ! -z $POLICY -a $POLICY != " " ] ; then
          echo "Adding policy '$POLICY' for '$USER'"
          ${MC} admin policy set myminio $POLICY user=$USER
      else
          echo "User '$USER' has no policy attached."
      fi
    }

    # Try connecting to MinIO instance
    scheme=http
    connectToMinio $scheme



    # Create the users
    echo console > $MINIO_ACCESSKEY_SECRETKEY_TMP
    echo console123 >> $MINIO_ACCESSKEY_SECRETKEY_TMP
    createUser consoleAdmin
  custom-command: |-
    #!/bin/sh
    set -e ; # Have script exit in the event of a failed command.
    MC_CONFIG_DIR="/tmp/minio/mc/"
    MC="/usr/bin/mc --insecure --config-dir ${MC_CONFIG_DIR}"

    # connectToMinio
    # Use a check-sleep-check loop to wait for MinIO service to be available
    connectToMinio() {
      SCHEME=$1
      ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts
      set -e ; # fail if we can't read the keys.
      ACCESS=$(cat /config/rootUser) ; SECRET=$(cat /config/rootPassword) ;
      set +e ; # The connections to minio are allowed to fail.
      echo "Connecting to MinIO server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT" ;
      MC_COMMAND="${MC} alias set myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET" ;
      $MC_COMMAND ;
      STATUS=$? ;
      until [ $STATUS = 0 ]
      do
        ATTEMPTS=`expr $ATTEMPTS + 1` ;
        echo \"Failed attempts: $ATTEMPTS\" ;
        if [ $ATTEMPTS -gt $LIMIT ]; then
          exit 1 ;
        fi ;
        sleep 2 ; # 1 second intervals between attempts
        $MC_COMMAND ;
        STATUS=$? ;
      done ;
      set -e ; # reset `e` as active
      return 0
    }

    # runCommand ($@)
    # Run custom mc command
    runCommand() {
      ${MC} "$@"
      return $?
    }

    # Try connecting to MinIO instance
    scheme=http
    connectToMinio $scheme
  initialize: "#!/bin/sh\nset -e ; # Have script exit in the event of a failed command.\nMC_CONFIG_DIR=\"/tmp/minio/mc/\"\nMC=\"/usr/bin/mc
    --insecure --config-dir ${MC_CONFIG_DIR}\"\n\n# connectToMinio\n# Use a check-sleep-check
    loop to wait for MinIO service to be available\nconnectToMinio() {\n  SCHEME=$1\n
    \ ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts\n  set -e ; # fail if we can't read
    the keys.\n  ACCESS=$(cat /config/rootUser) ; SECRET=$(cat /config/rootPassword)
    ;\n  set +e ; # The connections to minio are allowed to fail.\n  echo \"Connecting
    to MinIO server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT\" ;\n  MC_COMMAND=\"${MC}
    alias set myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET\" ;\n
    \ $MC_COMMAND ;\n  STATUS=$? ;\n  until [ $STATUS = 0 ]\n  do\n    ATTEMPTS=`expr
    $ATTEMPTS + 1` ;\n    echo \\\"Failed attempts: $ATTEMPTS\\\" ;\n    if [ $ATTEMPTS
    -gt $LIMIT ]; then\n      exit 1 ;\n    fi ;\n    sleep 2 ; # 1 second intervals
    between attempts\n    $MC_COMMAND ;\n    STATUS=$? ;\n  done ;\n  set -e ; # reset
    `e` as active\n  return 0\n}\n\n# checkBucketExists ($bucket)\n# Check if the
    bucket exists, by using the exit code of `mc ls`\ncheckBucketExists() {\n  BUCKET=$1\n
    \ CMD=$(${MC} stat myminio/$BUCKET > /dev/null 2>&1)\n  return $?\n}\n\n# createBucket
    ($bucket, $policy, $purge)\n# Ensure bucket exists, purging if asked to\ncreateBucket()
    {\n  BUCKET=$1\n  POLICY=$2\n  PURGE=$3\n  VERSIONING=$4\n  OBJECTLOCKING=$5\n\n
    \ # Purge the bucket, if set & exists\n  # Since PURGE is user input, check explicitly
    for `true`\n  if [ $PURGE = true ]; then\n    if checkBucketExists $BUCKET ; then\n
    \     echo \"Purging bucket '$BUCKET'.\"\n      set +e ; # don't exit if this
    fails\n      ${MC} rm -r --force myminio/$BUCKET\n      set -e ; # reset `e` as
    active\n    else\n      echo \"Bucket '$BUCKET' does not exist, skipping purge.\"\n
    \   fi\n  fi\n\n# Create the bucket if it does not exist and set objectlocking
    if enabled (NOTE: versioning will be not changed if OBJECTLOCKING is set because
    it enables versioning to the Buckets created)\nif ! checkBucketExists $BUCKET
    ; then\n    if [ ! -z $OBJECTLOCKING ] ; then\n      if [ $OBJECTLOCKING = true
    ] ; then\n          echo \"Creating bucket with OBJECTLOCKING '$BUCKET'\"\n          ${MC}
    mb --with-lock myminio/$BUCKET\n      elif [ $OBJECTLOCKING = false ] ; then\n
    \           echo \"Creating bucket '$BUCKET'\"\n            ${MC} mb myminio/$BUCKET\n
    \     fi\n  elif [ -z $OBJECTLOCKING ] ; then\n        echo \"Creating bucket
    '$BUCKET'\"\n        ${MC} mb myminio/$BUCKET\n  else\n    echo \"Bucket '$BUCKET'
    already exists.\"  \n  fi\n  fi\n\n\n  # set versioning for bucket if objectlocking
    is disabled or not set\n  if [ -z $OBJECTLOCKING ] ; then\n  if [ ! -z $VERSIONING
    ] ; then\n    if [ $VERSIONING = true ] ; then\n        echo \"Enabling versioning
    for '$BUCKET'\"\n        ${MC} version enable myminio/$BUCKET\n    elif [ $VERSIONING
    = false ] ; then\n        echo \"Suspending versioning for '$BUCKET'\"\n        ${MC}
    version suspend myminio/$BUCKET\n    fi\n    fi\n  else\n      echo \"Bucket '$BUCKET'
    versioning unchanged.\"\n  fi\n\n\n  # At this point, the bucket should exist,
    skip checking for existence\n  # Set policy on the bucket\n  echo \"Setting policy
    of bucket '$BUCKET' to '$POLICY'.\"\n  ${MC} anonymous set $POLICY myminio/$BUCKET\n}\n\n#
    Try connecting to MinIO instance\nscheme=http\nconnectToMinio $scheme\n\n\n\n#
    Create the buckets\ncreateBucket mimir-tsdb \"none\" false false false\ncreateBucket
    mimir-ruler \"none\" false false false\ncreateBucket enterprise-metrics-tsdb \"none\"
    false false false\ncreateBucket enterprise-metrics-admin \"none\" false false
    false\ncreateBucket enterprise-metrics-ruler \"none\" false false false"
kind: ConfigMap
metadata:
  labels:
    app: minio
    chart: minio-5.0.7
    heritage: Helm
    release: mimir
  name: mimir-minio
  namespace: monitoring
---
apiVersion: v1
data:
  nginx.conf: |
    worker_processes  5;  ## Default: 1
    error_log  /dev/stderr error;
    pid        /tmp/nginx.pid;
    worker_rlimit_nofile 8192;

    events {
      worker_connections  4096;  ## Default: 1024
    }

    http {
      client_body_temp_path /tmp/client_temp;
      proxy_temp_path       /tmp/proxy_temp_path;
      fastcgi_temp_path     /tmp/fastcgi_temp;
      uwsgi_temp_path       /tmp/uwsgi_temp;
      scgi_temp_path        /tmp/scgi_temp;

      default_type application/octet-stream;
      log_format   main '$remote_addr - $remote_user [$time_local]  $status '
            '"$request" $body_bytes_sent "$http_referer" '
            '"$http_user_agent" "$http_x_forwarded_for"';
      access_log   /dev/stderr  main;

      sendfile     on;
      tcp_nopush   on;
      resolver kube-dns.kube-system.svc.cluster.local;

      # Ensure that X-Scope-OrgID is always present, default to the no_auth_tenant for backwards compatibility when multi-tenancy was turned off.
      map $http_x_scope_orgid $ensured_x_scope_orgid {
        default $http_x_scope_orgid;
        "" "anonymous";
      }

      server {
        listen 8080;

        location = / {
          return 200 'OK';
          auth_basic off;
        }

        proxy_set_header X-Scope-OrgID $ensured_x_scope_orgid;

        # Distributor endpoints
        location /distributor {
          set $distributor mimir-distributor-headless.monitoring.svc.cluster.local;
          proxy_pass      http://$distributor:8080$request_uri;
        }
        location = /api/v1/push {
          set $distributor mimir-distributor-headless.monitoring.svc.cluster.local;
          proxy_pass      http://$distributor:8080$request_uri;
        }
        location /otlp/v1/metrics {
          set $distributor mimir-distributor-headless.monitoring.svc.cluster.local;
          proxy_pass      http://$distributor:8080$request_uri;
        }

        # Alertmanager endpoints
        location /alertmanager {
          set $alertmanager mimir-alertmanager-headless.monitoring.svc.cluster.local;
          proxy_pass      http://$alertmanager:8080$request_uri;
        }
        location = /multitenant_alertmanager/status {
          set $alertmanager mimir-alertmanager-headless.monitoring.svc.cluster.local;
          proxy_pass      http://$alertmanager:8080$request_uri;
        }
        location = /api/v1/alerts {
          set $alertmanager mimir-alertmanager-headless.monitoring.svc.cluster.local;
          proxy_pass      http://$alertmanager:8080$request_uri;
        }

        # Ruler endpoints
        location /prometheus/config/v1/rules {
          set $ruler mimir-ruler.monitoring.svc.cluster.local;
          proxy_pass      http://$ruler:8080$request_uri;
        }
        location /prometheus/api/v1/rules {
          set $ruler mimir-ruler.monitoring.svc.cluster.local;
          proxy_pass      http://$ruler:8080$request_uri;
        }

        location /prometheus/api/v1/alerts {
          set $ruler mimir-ruler.monitoring.svc.cluster.local;
          proxy_pass      http://$ruler:8080$request_uri;
        }
        location = /ruler/ring {
          set $ruler mimir-ruler.monitoring.svc.cluster.local;
          proxy_pass      http://$ruler:8080$request_uri;
        }

        # Rest of /prometheus goes to the query frontend
        location /prometheus {
          set $query_frontend mimir-query-frontend.monitoring.svc.cluster.local;
          proxy_pass      http://$query_frontend:8080$request_uri;
        }

        # Buildinfo endpoint can go to any component
        location = /api/v1/status/buildinfo {
          set $query_frontend mimir-query-frontend.monitoring.svc.cluster.local;
          proxy_pass      http://$query_frontend:8080$request_uri;
        }

        # Compactor endpoint for uploading blocks
        location /api/v1/upload/block/ {
          set $compactor mimir-compactor.monitoring.svc.cluster.local;
          proxy_pass      http://$compactor:8080$request_uri;
        }
      }
    }
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: nginx
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/version: 2.8.0
    helm.sh/chart: mimir-distributed-4.4.1
  name: mimir-nginx
  namespace: monitoring
---
apiVersion: v1
data:
  runtime.yaml: |2

    {}
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/version: 2.8.0
    helm.sh/chart: mimir-distributed-4.4.1
  name: mimir-runtime
  namespace: monitoring
---
apiVersion: v1
data:
  rootPassword: c3VwZXJzZWNyZXQ=
  rootUser: Z3JhZmFuYS1taW1pcg==
kind: Secret
metadata:
  labels:
    app: minio
    chart: minio-5.0.7
    heritage: Helm
    release: mimir
  name: mimir-minio
  namespace: monitoring
type: Opaque
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: grafana
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana
    app.kubernetes.io/version: 9.5.1
    helm.sh/chart: grafana-6.56.2
  name: grafana
  namespace: monitoring
spec:
  ports:
  - name: service
    port: 80
    protocol: TCP
    targetPort: 3000
  selector:
    app.kubernetes.io/instance: grafana
    app.kubernetes.io/name: grafana
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: grafana-agent
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana-agent
    app.kubernetes.io/version: v0.33.1
    helm.sh/chart: grafana-agent-0.13.0
  name: grafana-agent
  namespace: monitoring
spec:
  ports:
  - name: http-metrics
    port: 80
    protocol: TCP
    targetPort: 80
  - name: faro
    port: 12347
    protocol: TCP
    targetPort: 12347
  selector:
    app.kubernetes.io/instance: grafana-agent
    app.kubernetes.io/name: grafana-agent
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: metrics
    app.kubernetes.io/instance: kube-state-metrics
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/version: 2.8.2
    helm.sh/chart: kube-state-metrics-5.6.1
  name: kube-state-metrics
  namespace: monitoring
spec:
  ports:
  - name: http
    port: 8080
    protocol: TCP
    targetPort: 8080
  - name: metrics
    port: 8081
    protocol: TCP
    targetPort: 8081
  selector:
    app.kubernetes.io/instance: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: loki
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: loki
    app.kubernetes.io/version: 2.8.2
    helm.sh/chart: loki-5.5.0
  name: loki
  namespace: monitoring
spec:
  ports:
  - name: http-metrics
    port: 3100
    protocol: TCP
    targetPort: http-metrics
  - name: grpc
    port: 9095
    protocol: TCP
    targetPort: grpc
  selector:
    app.kubernetes.io/component: single-binary
    app.kubernetes.io/instance: loki
    app.kubernetes.io/name: loki
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: canary
    app.kubernetes.io/instance: loki
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: loki
    app.kubernetes.io/version: 2.8.2
    helm.sh/chart: loki-5.5.0
  name: loki-canary
  namespace: monitoring
spec:
  ports:
  - name: http-metrics
    port: 3500
    protocol: TCP
    targetPort: http-metrics
  selector:
    app.kubernetes.io/component: canary
    app.kubernetes.io/instance: loki
    app.kubernetes.io/name: loki
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: gateway
    app.kubernetes.io/instance: loki
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: loki
    app.kubernetes.io/version: 2.8.2
    helm.sh/chart: loki-5.5.0
  name: loki-gateway
  namespace: monitoring
spec:
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: http
  selector:
    app.kubernetes.io/component: gateway
    app.kubernetes.io/instance: loki
    app.kubernetes.io/name: loki
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: loki
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: loki
    app.kubernetes.io/version: 2.8.2
    helm.sh/chart: loki-5.5.0
    prometheus.io/service-monitor: "false"
    variant: headless
  name: loki-headless
  namespace: monitoring
spec:
  clusterIP: None
  ports:
  - name: http-metrics
    port: 3100
    protocol: TCP
    targetPort: http-metrics
  selector:
    app.kubernetes.io/instance: loki
    app.kubernetes.io/name: loki
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: loki
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: loki
    app.kubernetes.io/version: 2.8.2
    helm.sh/chart: loki-5.5.0
  name: loki-memberlist
  namespace: monitoring
spec:
  clusterIP: None
  ports:
  - name: tcp
    port: 7946
    protocol: TCP
    targetPort: http-memberlist
  selector:
    app.kubernetes.io/instance: loki
    app.kubernetes.io/name: loki
    app.kubernetes.io/part-of: memberlist
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: alertmanager
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: 2.8.0
    helm.sh/chart: mimir-distributed-4.4.1
  name: mimir-alertmanager
  namespace: monitoring
spec:
  ports:
  - name: http-metrics
    port: 8080
    protocol: TCP
    targetPort: http-metrics
  - name: grpc
    port: 9095
    protocol: TCP
    targetPort: grpc
  selector:
    app.kubernetes.io/component: alertmanager
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/name: mimir
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: alertmanager
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: 2.8.0
    helm.sh/chart: mimir-distributed-4.4.1
    prometheus.io/service-monitor: "false"
  name: mimir-alertmanager-headless
  namespace: monitoring
spec:
  clusterIP: None
  ports:
  - name: http-metrics
    port: 8080
    protocol: TCP
    targetPort: http-metrics
  - name: grpc
    port: 9095
    protocol: TCP
    targetPort: grpc
  - name: cluster
    port: 9094
    protocol: TCP
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/component: alertmanager
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/name: mimir
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: compactor
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: 2.8.0
    helm.sh/chart: mimir-distributed-4.4.1
  name: mimir-compactor
  namespace: monitoring
spec:
  ports:
  - name: http-metrics
    port: 8080
    protocol: TCP
    targetPort: http-metrics
  - name: grpc
    port: 9095
    protocol: TCP
    targetPort: grpc
  selector:
    app.kubernetes.io/component: compactor
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/name: mimir
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: distributor
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: 2.8.0
    helm.sh/chart: mimir-distributed-4.4.1
  name: mimir-distributor
  namespace: monitoring
spec:
  ports:
  - name: http-metrics
    port: 8080
    protocol: TCP
    targetPort: http-metrics
  - name: grpc
    port: 9095
    protocol: TCP
    targetPort: grpc
  selector:
    app.kubernetes.io/component: distributor
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/name: mimir
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: distributor
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: 2.8.0
    helm.sh/chart: mimir-distributed-4.4.1
    prometheus.io/service-monitor: "false"
  name: mimir-distributor-headless
  namespace: monitoring
spec:
  clusterIP: None
  ports:
  - name: http-metrics
    port: 8080
    protocol: TCP
    targetPort: http-metrics
  - name: grpc
    port: 9095
    protocol: TCP
    targetPort: grpc
  selector:
    app.kubernetes.io/component: distributor
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/name: mimir
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: gossip-ring
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/version: 2.8.0
    helm.sh/chart: mimir-distributed-4.4.1
  name: mimir-gossip-ring
  namespace: monitoring
spec:
  clusterIP: None
  ports:
  - name: gossip-ring
    port: 7946
    protocol: TCP
    targetPort: 7946
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/name: mimir
    app.kubernetes.io/part-of: memberlist
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: ingester
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: 2.8.0
    helm.sh/chart: mimir-distributed-4.4.1
  name: mimir-ingester
  namespace: monitoring
spec:
  ports:
  - name: http-metrics
    port: 8080
    protocol: TCP
    targetPort: http-metrics
  - name: grpc
    port: 9095
    protocol: TCP
    targetPort: grpc
  selector:
    app.kubernetes.io/component: ingester
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/name: mimir
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: ingester
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: 2.8.0
    helm.sh/chart: mimir-distributed-4.4.1
    prometheus.io/service-monitor: "false"
  name: mimir-ingester-headless
  namespace: monitoring
spec:
  clusterIP: None
  ports:
  - name: http-metrics
    port: 8080
    protocol: TCP
    targetPort: http-metrics
  - name: grpc
    port: 9095
    protocol: TCP
    targetPort: grpc
  selector:
    app.kubernetes.io/component: ingester
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/name: mimir
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: minio
    chart: minio-5.0.7
    heritage: Helm
    monitoring: "true"
    release: mimir
  name: mimir-minio
  namespace: monitoring
spec:
  ports:
  - name: http
    port: 9000
    protocol: TCP
    targetPort: 9000
  selector:
    app: minio
    release: mimir
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: minio
    chart: minio-5.0.7
    heritage: Helm
    release: mimir
  name: mimir-minio-console
  namespace: monitoring
spec:
  ports:
  - name: http
    port: 9001
    protocol: TCP
    targetPort: 9001
  selector:
    app: minio
    release: mimir
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: nginx
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/version: 2.8.0
    helm.sh/chart: mimir-distributed-4.4.1
  name: mimir-nginx
  namespace: monitoring
spec:
  ports:
  - name: http-metric
    port: 80
    protocol: TCP
    targetPort: http-metric
  selector:
    app.kubernetes.io/component: nginx
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/name: mimir
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: overrides-exporter
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/version: 2.8.0
    helm.sh/chart: mimir-distributed-4.4.1
  name: mimir-overrides-exporter
  namespace: monitoring
spec:
  ports:
  - name: http-metrics
    port: 8080
    protocol: TCP
    targetPort: http-metrics
  - name: grpc
    port: 9095
    protocol: TCP
    targetPort: grpc
  selector:
    app.kubernetes.io/component: overrides-exporter
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/name: mimir
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: querier
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: 2.8.0
    helm.sh/chart: mimir-distributed-4.4.1
  name: mimir-querier
  namespace: monitoring
spec:
  ports:
  - name: http-metrics
    port: 8080
    protocol: TCP
    targetPort: http-metrics
  - name: grpc
    port: 9095
    protocol: TCP
    targetPort: grpc
  selector:
    app.kubernetes.io/component: querier
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/name: mimir
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: query-frontend
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/version: 2.8.0
    helm.sh/chart: mimir-distributed-4.4.1
  name: mimir-query-frontend
  namespace: monitoring
spec:
  ports:
  - name: http-metrics
    port: 8080
    protocol: TCP
    targetPort: http-metrics
  - name: grpc
    port: 9095
    protocol: TCP
    targetPort: grpc
  selector:
    app.kubernetes.io/component: query-frontend
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/name: mimir
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: query-scheduler
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/version: 2.8.0
    helm.sh/chart: mimir-distributed-4.4.1
  name: mimir-query-scheduler
  namespace: monitoring
spec:
  ports:
  - name: http-metrics
    port: 8080
    protocol: TCP
    targetPort: http-metrics
  - name: grpc
    port: 9095
    protocol: TCP
    targetPort: grpc
  selector:
    app.kubernetes.io/component: query-scheduler
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/name: mimir
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: query-scheduler
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/version: 2.8.0
    helm.sh/chart: mimir-distributed-4.4.1
    prometheus.io/service-monitor: "false"
  name: mimir-query-scheduler-headless
  namespace: monitoring
spec:
  clusterIP: None
  ports:
  - name: http-metrics
    port: 8080
    protocol: TCP
    targetPort: http-metrics
  - name: grpc
    port: 9095
    protocol: TCP
    targetPort: grpc
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/component: query-scheduler
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/name: mimir
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: ruler
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: 2.8.0
    helm.sh/chart: mimir-distributed-4.4.1
  name: mimir-ruler
  namespace: monitoring
spec:
  ports:
  - name: http-metrics
    port: 8080
    protocol: TCP
    targetPort: http-metrics
  selector:
    app.kubernetes.io/component: ruler
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/name: mimir
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: store-gateway
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: 2.8.0
    helm.sh/chart: mimir-distributed-4.4.1
  name: mimir-store-gateway
  namespace: monitoring
spec:
  ports:
  - name: http-metrics
    port: 8080
    protocol: TCP
    targetPort: http-metrics
  - name: grpc
    port: 9095
    protocol: TCP
    targetPort: grpc
  selector:
    app.kubernetes.io/component: store-gateway
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/name: mimir
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: store-gateway
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: 2.8.0
    helm.sh/chart: mimir-distributed-4.4.1
    prometheus.io/service-monitor: "false"
  name: mimir-store-gateway-headless
  namespace: monitoring
spec:
  clusterIP: None
  ports:
  - name: http-metrics
    port: 8080
    protocol: TCP
    targetPort: http-metrics
  - name: grpc
    port: 9095
    protocol: TCP
    targetPort: grpc
  selector:
    app.kubernetes.io/component: store-gateway
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/name: mimir
  type: ClusterIP
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  finalizers:
  - kubernetes.io/pvc-protection
  labels:
    app.kubernetes.io/instance: grafana
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana
    app.kubernetes.io/version: 9.5.1
    helm.sh/chart: grafana-6.56.2
  name: grafana
  namespace: monitoring
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    app: minio
    chart: minio-5.0.7
    heritage: Helm
    release: mimir
  name: mimir-minio
  namespace: monitoring
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: grafana
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana
    app.kubernetes.io/version: 9.5.1
    helm.sh/chart: grafana-6.56.2
  name: grafana
  namespace: monitoring
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/instance: grafana
      app.kubernetes.io/name: grafana
  strategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/config: 7b230f3843238349df86fe570f14f89b79bfd7eeba4dba094ff40ff5ab252e69
        checksum/dashboards-json-config: 880f5c43531b7fb2e84c481d47c7c98bccd069ce7fe7f3d3c931a84b98ef5715
        checksum/sc-dashboard-provider-config: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        kubectl.kubernetes.io/default-container: grafana
      labels:
        app.kubernetes.io/instance: grafana
        app.kubernetes.io/name: grafana
    spec:
      automountServiceAccountToken: true
      containers:
      - env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: GF_SECURITY_ADMIN_USER
          valueFrom:
            secretKeyRef:
              key: admin-user
              name: grafana
        - name: GF_SECURITY_ADMIN_PASSWORD
          valueFrom:
            secretKeyRef:
              key: admin-password
              name: grafana
        - name: GF_PATHS_DATA
          value: /var/lib/grafana/
        - name: GF_PATHS_LOGS
          value: /var/log/grafana
        - name: GF_PATHS_PLUGINS
          value: /var/lib/grafana/plugins
        - name: GF_PATHS_PROVISIONING
          value: /etc/grafana/provisioning
        image: docker.io/grafana/grafana:9.5.1
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 10
          httpGet:
            path: /api/health
            port: 3000
          initialDelaySeconds: 60
          timeoutSeconds: 30
        name: grafana
        ports:
        - containerPort: 3000
          name: grafana
          protocol: TCP
        - containerPort: 9094
          name: gossip-tcp
          protocol: TCP
        - containerPort: 9094
          name: gossip-udp
          protocol: UDP
        readinessProbe:
          httpGet:
            path: /api/health
            port: 3000
        securityContext:
          capabilities:
            drop:
            - ALL
          seccompProfile:
            type: RuntimeDefault
        volumeMounts:
        - mountPath: /etc/grafana/grafana.ini
          name: config
          subPath: grafana.ini
        - mountPath: /var/lib/grafana
          name: storage
        - mountPath: /etc/grafana/provisioning/datasources/datasources.yaml
          name: config
          subPath: datasources.yaml
        - mountPath: /etc/grafana/provisioning/dashboards/dashboardproviders.yaml
          name: config
          subPath: dashboardproviders.yaml
      enableServiceLinks: true
      initContainers:
      - command:
        - chown
        - -R
        - 472:472
        - /var/lib/grafana
        image: docker.io/library/busybox:1.31.1
        imagePullPolicy: IfNotPresent
        name: init-chown-data
        securityContext:
          capabilities:
            add:
            - CHOWN
          runAsNonRoot: false
          runAsUser: 0
          seccompProfile:
            type: RuntimeDefault
        volumeMounts:
        - mountPath: /var/lib/grafana
          name: storage
      - args:
        - -c
        - mkdir -p /var/lib/grafana/dashboards/default && /bin/sh -x /etc/grafana/download_dashboards.sh
        command:
        - /bin/sh
        env: null
        image: docker.io/curlimages/curl:7.85.0
        imagePullPolicy: IfNotPresent
        name: download-dashboards
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          seccompProfile:
            type: RuntimeDefault
        volumeMounts:
        - mountPath: /etc/grafana/download_dashboards.sh
          name: config
          subPath: download_dashboards.sh
        - mountPath: /var/lib/grafana
          name: storage
      securityContext:
        fsGroup: 472
        runAsGroup: 472
        runAsNonRoot: true
        runAsUser: 472
      serviceAccountName: grafana
      volumes:
      - configMap:
          name: grafana
        name: config
      - configMap:
          name: grafana-dashboards-grafana-dashboards-kubernetes
        name: dashboards-grafana-dashboards-kubernetes
      - name: storage
        persistentVolumeClaim:
          claimName: grafana
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: metrics
    app.kubernetes.io/instance: kube-state-metrics
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/version: 2.8.2
    helm.sh/chart: kube-state-metrics-5.6.1
  name: kube-state-metrics
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: kube-state-metrics
      app.kubernetes.io/name: kube-state-metrics
  template:
    metadata:
      labels:
        app.kubernetes.io/component: metrics
        app.kubernetes.io/instance: kube-state-metrics
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: kube-state-metrics
        app.kubernetes.io/part-of: kube-state-metrics
        app.kubernetes.io/version: 2.8.2
        helm.sh/chart: kube-state-metrics-5.6.1
    spec:
      containers:
      - args:
        - --port=8080
        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
        image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.8.2
        imagePullPolicy: IfNotPresent
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 5
          timeoutSeconds: 5
        name: kube-state-metrics
        ports:
        - containerPort: 8080
          name: http
        - containerPort: 8081
          name: metrics
        readinessProbe:
          httpGet:
            path: /
            port: 8080
          initialDelaySeconds: 5
          timeoutSeconds: 5
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
      hostNetwork: false
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: kube-state-metrics
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: gateway
    app.kubernetes.io/instance: loki
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: loki
    app.kubernetes.io/version: 2.8.2
    helm.sh/chart: loki-5.5.0
  name: loki-gateway
  namespace: monitoring
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/component: gateway
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
  strategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/config: 38b6849fe32ccc7fb2f20c5a279ee62f80ec31d0192e93e22befb9876a0e7c6f
      labels:
        app.kubernetes.io/component: gateway
        app.kubernetes.io/instance: loki
        app.kubernetes.io/name: loki
    spec:
      containers:
      - image: docker.io/nginxinc/nginx-unprivileged:1.19-alpine
        imagePullPolicy: IfNotPresent
        name: nginx
        ports:
        - containerPort: 8080
          name: http
          protocol: TCP
        readinessProbe:
          httpGet:
            path: /
            port: http
          initialDelaySeconds: 15
          timeoutSeconds: 1
        resources: {}
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
        volumeMounts:
        - mountPath: /etc/nginx
          name: config
        - mountPath: /tmp
          name: tmp
        - mountPath: /docker-entrypoint.d
          name: docker-entrypoint-d-override
      enableServiceLinks: true
      securityContext:
        fsGroup: 101
        runAsGroup: 101
        runAsNonRoot: true
        runAsUser: 101
      serviceAccountName: loki
      terminationGracePeriodSeconds: 30
      volumes:
      - configMap:
          name: loki-gateway
        name: config
      - emptyDir: {}
        name: tmp
      - emptyDir: {}
        name: docker-entrypoint-d-override
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: distributor
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: 2.8.0
    helm.sh/chart: mimir-distributed-4.4.1
  name: mimir-distributor
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: distributor
      app.kubernetes.io/instance: mimir
      app.kubernetes.io/name: mimir
  strategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/config: d816162f2de5bc934e373c238e23069f055826ea2c378c29c0d1941356b87873
      labels:
        app.kubernetes.io/component: distributor
        app.kubernetes.io/instance: mimir
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: mimir
        app.kubernetes.io/part-of: memberlist
        app.kubernetes.io/version: 2.8.0
        helm.sh/chart: mimir-distributed-4.4.1
      namespace: monitoring
    spec:
      affinity: {}
      containers:
      - args:
        - -target=distributor
        - -config.expand-env=true
        - -config.file=/etc/mimir/mimir.yaml
        env: null
        envFrom: null
        image: grafana/mimir:2.8.0
        imagePullPolicy: IfNotPresent
        livenessProbe: null
        name: distributor
        ports:
        - containerPort: 8080
          name: http-metrics
          protocol: TCP
        - containerPort: 9095
          name: grpc
          protocol: TCP
        - containerPort: 7946
          name: memberlist
          protocol: TCP
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 45
        resources:
          requests:
            cpu: 100m
            memory: 512Mi
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
        volumeMounts:
        - mountPath: /etc/mimir
          name: config
        - mountPath: /var/mimir
          name: runtime-config
        - mountPath: /data
          name: storage
          subPath: null
        - mountPath: /active-query-tracker
          name: active-queries
      initContainers: []
      nodeSelector: {}
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: mimir
      terminationGracePeriodSeconds: 60
      tolerations: []
      topologySpreadConstraints:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/component: distributor
            app.kubernetes.io/instance: mimir
            app.kubernetes.io/name: mimir
        maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
      volumes:
      - configMap:
          items:
          - key: mimir.yaml
            path: mimir.yaml
          name: mimir-config
        name: config
      - configMap:
          name: mimir-runtime
        name: runtime-config
      - emptyDir: {}
        name: storage
      - emptyDir: {}
        name: active-queries
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: minio
    chart: minio-5.0.7
    heritage: Helm
    release: mimir
  name: mimir-minio
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: minio
      release: mimir
  strategy:
    rollingUpdate:
      maxSurge: 100%
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/config: ff11b162c0e4cf4d5b55aafd6e9cfdec1558255d658a6566d94a859d57d2bb65
        checksum/secrets: 17f2fb9f843a199d0c8a7e2ad4f23563898695023bed3989d9017a308ec1ce05
      labels:
        app: minio
        release: mimir
      name: mimir-minio
    spec:
      containers:
      - command:
        - /bin/sh
        - -ce
        - /usr/bin/docker-entrypoint.sh minio server /export -S /etc/minio/certs/
          --address :9000 --console-address :9001
        env:
        - name: MINIO_ROOT_USER
          valueFrom:
            secretKeyRef:
              key: rootUser
              name: mimir-minio
        - name: MINIO_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              key: rootPassword
              name: mimir-minio
        - name: MINIO_PROMETHEUS_AUTH_TYPE
          value: public
        image: quay.io/minio/minio:RELEASE.2023-02-10T18-48-39Z
        imagePullPolicy: IfNotPresent
        name: minio
        ports:
        - containerPort: 9000
          name: http
        - containerPort: 9001
          name: http-console
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
        volumeMounts:
        - mountPath: /tmp/credentials
          name: minio-user
          readOnly: true
        - mountPath: /export
          name: export
      securityContext:
        fsGroup: 1000
        fsGroupChangePolicy: OnRootMismatch
        runAsGroup: 1000
        runAsUser: 1000
      serviceAccountName: minio-sa
      volumes:
      - name: export
        persistentVolumeClaim:
          claimName: mimir-minio
      - name: minio-user
        secret:
          secretName: mimir-minio
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: nginx
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/version: 2.8.0
    helm.sh/chart: mimir-distributed-4.4.1
  name: mimir-nginx
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: nginx
      app.kubernetes.io/instance: mimir
      app.kubernetes.io/name: mimir
  strategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/config: 9f8388370e872540f37b6b929c3c594809538530a0d2656f54b7be88ef26679b
      labels:
        app.kubernetes.io/component: nginx
        app.kubernetes.io/instance: mimir
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: mimir
        app.kubernetes.io/version: 2.8.0
        helm.sh/chart: mimir-distributed-4.4.1
      namespace: monitoring
    spec:
      containers:
      - env: null
        envFrom: null
        image: docker.io/nginxinc/nginx-unprivileged:1.22-alpine
        imagePullPolicy: IfNotPresent
        name: nginx
        ports:
        - containerPort: 8080
          name: http-metric
          protocol: TCP
        readinessProbe:
          httpGet:
            path: /
            port: http-metric
          initialDelaySeconds: 15
          timeoutSeconds: 1
        resources: {}
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
        volumeMounts:
        - mountPath: /etc/nginx
          name: config
        - mountPath: /tmp
          name: tmp
        - mountPath: /docker-entrypoint.d
          name: docker-entrypoint-d-override
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: mimir
      terminationGracePeriodSeconds: 30
      topologySpreadConstraints:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/component: nginx
            app.kubernetes.io/instance: mimir
            app.kubernetes.io/name: mimir
        maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
      volumes:
      - configMap:
          name: mimir-nginx
        name: config
      - emptyDir: {}
        name: tmp
      - emptyDir: {}
        name: docker-entrypoint-d-override
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: overrides-exporter
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/version: 2.8.0
    helm.sh/chart: mimir-distributed-4.4.1
  name: mimir-overrides-exporter
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: overrides-exporter
      app.kubernetes.io/instance: mimir
      app.kubernetes.io/name: mimir
  strategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/config: d816162f2de5bc934e373c238e23069f055826ea2c378c29c0d1941356b87873
      labels:
        app.kubernetes.io/component: overrides-exporter
        app.kubernetes.io/instance: mimir
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: mimir
        app.kubernetes.io/version: 2.8.0
        helm.sh/chart: mimir-distributed-4.4.1
      namespace: monitoring
    spec:
      affinity: {}
      containers:
      - args:
        - -target=overrides-exporter
        - -config.expand-env=true
        - -config.file=/etc/mimir/mimir.yaml
        env: null
        envFrom: null
        image: grafana/mimir:2.8.0
        imagePullPolicy: IfNotPresent
        livenessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 45
        name: overrides-exporter
        ports:
        - containerPort: 8080
          name: http-metrics
          protocol: TCP
        - containerPort: 9095
          name: grpc
          protocol: TCP
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 45
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
        volumeMounts:
        - mountPath: /etc/mimir
          name: config
        - mountPath: /var/mimir
          name: runtime-config
        - mountPath: /data
          name: storage
          subPath: null
        - mountPath: /active-query-tracker
          name: active-queries
      initContainers: []
      nodeSelector: {}
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: mimir
      terminationGracePeriodSeconds: 60
      tolerations: []
      volumes:
      - configMap:
          items:
          - key: mimir.yaml
            path: mimir.yaml
          name: mimir-config
        name: config
      - configMap:
          name: mimir-runtime
        name: runtime-config
      - emptyDir: {}
        name: storage
      - emptyDir: {}
        name: active-queries
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: querier
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: 2.8.0
    helm.sh/chart: mimir-distributed-4.4.1
  name: mimir-querier
  namespace: monitoring
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/component: querier
      app.kubernetes.io/instance: mimir
      app.kubernetes.io/name: mimir
  strategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/config: d816162f2de5bc934e373c238e23069f055826ea2c378c29c0d1941356b87873
      labels:
        app.kubernetes.io/component: querier
        app.kubernetes.io/instance: mimir
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: mimir
        app.kubernetes.io/part-of: memberlist
        app.kubernetes.io/version: 2.8.0
        helm.sh/chart: mimir-distributed-4.4.1
    spec:
      affinity: {}
      containers:
      - args:
        - -target=querier
        - -config.expand-env=true
        - -config.file=/etc/mimir/mimir.yaml
        env: null
        envFrom: null
        image: grafana/mimir:2.8.0
        imagePullPolicy: IfNotPresent
        livenessProbe: null
        name: querier
        ports:
        - containerPort: 8080
          name: http-metrics
          protocol: TCP
        - containerPort: 9095
          name: grpc
          protocol: TCP
        - containerPort: 7946
          name: memberlist
          protocol: TCP
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 45
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
        volumeMounts:
        - mountPath: /etc/mimir
          name: config
        - mountPath: /var/mimir
          name: runtime-config
        - mountPath: /data
          name: storage
          subPath: null
        - mountPath: /active-query-tracker
          name: active-queries
      initContainers: []
      nodeSelector: {}
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: mimir
      terminationGracePeriodSeconds: 180
      tolerations: []
      topologySpreadConstraints:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/component: querier
            app.kubernetes.io/instance: mimir
            app.kubernetes.io/name: mimir
        maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
      volumes:
      - configMap:
          items:
          - key: mimir.yaml
            path: mimir.yaml
          name: mimir-config
        name: config
      - configMap:
          name: mimir-runtime
        name: runtime-config
      - emptyDir: {}
        name: storage
      - emptyDir: {}
        name: active-queries
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: query-frontend
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/version: 2.8.0
    helm.sh/chart: mimir-distributed-4.4.1
  name: mimir-query-frontend
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: query-frontend
      app.kubernetes.io/instance: mimir
      app.kubernetes.io/name: mimir
  strategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/config: d816162f2de5bc934e373c238e23069f055826ea2c378c29c0d1941356b87873
      labels:
        app.kubernetes.io/component: query-frontend
        app.kubernetes.io/instance: mimir
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: mimir
        app.kubernetes.io/version: 2.8.0
        helm.sh/chart: mimir-distributed-4.4.1
      namespace: monitoring
    spec:
      affinity: {}
      containers:
      - args:
        - -target=query-frontend
        - -config.expand-env=true
        - -config.file=/etc/mimir/mimir.yaml
        env: null
        envFrom: null
        image: grafana/mimir:2.8.0
        imagePullPolicy: IfNotPresent
        livenessProbe: null
        name: query-frontend
        ports:
        - containerPort: 8080
          name: http-metrics
          protocol: TCP
        - containerPort: 9095
          name: grpc
          protocol: TCP
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 45
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
        volumeMounts:
        - mountPath: /var/mimir
          name: runtime-config
        - mountPath: /etc/mimir
          name: config
        - mountPath: /data
          name: storage
        - mountPath: /active-query-tracker
          name: active-queries
      initContainers: []
      nodeSelector: {}
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: mimir
      terminationGracePeriodSeconds: 180
      tolerations: []
      topologySpreadConstraints:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/component: query-frontend
            app.kubernetes.io/instance: mimir
            app.kubernetes.io/name: mimir
        maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
      volumes:
      - configMap:
          items:
          - key: mimir.yaml
            path: mimir.yaml
          name: mimir-config
        name: config
      - configMap:
          name: mimir-runtime
        name: runtime-config
      - emptyDir: {}
        name: storage
      - emptyDir: {}
        name: active-queries
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: query-scheduler
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/version: 2.8.0
    helm.sh/chart: mimir-distributed-4.4.1
  name: mimir-query-scheduler
  namespace: monitoring
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/component: query-scheduler
      app.kubernetes.io/instance: mimir
      app.kubernetes.io/name: mimir
  strategy:
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/config: d816162f2de5bc934e373c238e23069f055826ea2c378c29c0d1941356b87873
      labels:
        app.kubernetes.io/component: query-scheduler
        app.kubernetes.io/instance: mimir
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: mimir
        app.kubernetes.io/version: 2.8.0
        helm.sh/chart: mimir-distributed-4.4.1
    spec:
      affinity: {}
      containers:
      - args:
        - -target=query-scheduler
        - -config.expand-env=true
        - -config.file=/etc/mimir/mimir.yaml
        - -server.grpc.keepalive.max-connection-age=2562047h
        - -server.grpc.keepalive.max-connection-age-grace=2562047h
        env: null
        envFrom: null
        image: grafana/mimir:2.8.0
        imagePullPolicy: IfNotPresent
        livenessProbe: null
        name: query-scheduler
        ports:
        - containerPort: 8080
          name: http-metrics
          protocol: TCP
        - containerPort: 9095
          name: grpc
          protocol: TCP
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 45
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
        volumeMounts:
        - mountPath: /var/mimir
          name: runtime-config
        - mountPath: /etc/mimir
          name: config
        - mountPath: /data
          name: storage
        - mountPath: /active-query-tracker
          name: active-queries
      initContainers: []
      nodeSelector: {}
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: mimir
      terminationGracePeriodSeconds: 180
      tolerations: []
      topologySpreadConstraints:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/component: query-scheduler
            app.kubernetes.io/instance: mimir
            app.kubernetes.io/name: mimir
        maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
      volumes:
      - configMap:
          items:
          - key: mimir.yaml
            path: mimir.yaml
          name: mimir-config
        name: config
      - configMap:
          name: mimir-runtime
        name: runtime-config
      - emptyDir: {}
        name: storage
      - emptyDir: {}
        name: active-queries
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: rollout-operator
    app.kubernetes.io/version: v0.4.0
    helm.sh/chart: rollout-operator-0.4.2
  name: mimir-rollout-operator
  namespace: monitoring
spec:
  minReadySeconds: 10
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: mimir
      app.kubernetes.io/name: rollout-operator
  strategy:
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: mimir
        app.kubernetes.io/name: rollout-operator
    spec:
      containers:
      - args:
        - -kubernetes.namespace=monitoring
        image: grafana/rollout-operator:v0.4.0
        imagePullPolicy: IfNotPresent
        name: rollout-operator
        ports:
        - containerPort: 8001
          name: http-metrics
          protocol: TCP
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 5
          timeoutSeconds: 1
        resources:
          limits:
            cpu: "1"
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 100Mi
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: mimir-rollout-operator
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: ruler
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: 2.8.0
    helm.sh/chart: mimir-distributed-4.4.1
  name: mimir-ruler
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: ruler
      app.kubernetes.io/instance: mimir
      app.kubernetes.io/name: mimir
  strategy:
    rollingUpdate:
      maxSurge: 50%
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/config: d816162f2de5bc934e373c238e23069f055826ea2c378c29c0d1941356b87873
      labels:
        app.kubernetes.io/component: ruler
        app.kubernetes.io/instance: mimir
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: mimir
        app.kubernetes.io/part-of: memberlist
        app.kubernetes.io/version: 2.8.0
        helm.sh/chart: mimir-distributed-4.4.1
      namespace: monitoring
    spec:
      affinity: {}
      containers:
      - args:
        - -target=ruler
        - -config.expand-env=true
        - -config.file=/etc/mimir/mimir.yaml
        env: null
        envFrom: null
        image: grafana/mimir:2.8.0
        imagePullPolicy: IfNotPresent
        livenessProbe: null
        name: ruler
        ports:
        - containerPort: 8080
          name: http-metrics
          protocol: TCP
        - containerPort: 9095
          name: grpc
          protocol: TCP
        - containerPort: 7946
          name: memberlist
          protocol: TCP
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 45
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
        volumeMounts:
        - mountPath: /etc/mimir
          name: config
        - mountPath: /var/mimir
          name: runtime-config
        - mountPath: /data
          name: storage
          subPath: null
        - mountPath: /active-query-tracker
          name: active-queries
      initContainers: []
      nodeSelector: {}
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: mimir
      terminationGracePeriodSeconds: 180
      tolerations: []
      topologySpreadConstraints:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/component: ruler
            app.kubernetes.io/instance: mimir
            app.kubernetes.io/name: mimir
        maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
      volumes:
      - configMap:
          items:
          - key: mimir.yaml
            path: mimir.yaml
          name: mimir-config
        name: config
      - configMap:
          name: mimir-runtime
        name: runtime-config
      - emptyDir: {}
        name: storage
      - emptyDir: {}
        name: active-queries
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: single-binary
    app.kubernetes.io/instance: loki
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: loki
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: 2.8.2
    helm.sh/chart: loki-5.5.0
  name: loki
  namespace: monitoring
spec:
  podManagementPolicy: Parallel
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/component: single-binary
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
  serviceName: loki-headless
  template:
    metadata:
      annotations:
        checksum/config: 74301122ecaf278ef02c59da3f352da60cfdaf387383f7b0caaddde868b165d6
      labels:
        app.kubernetes.io/component: single-binary
        app.kubernetes.io/instance: loki
        app.kubernetes.io/name: loki
        app.kubernetes.io/part-of: memberlist
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/component: single-binary
                app.kubernetes.io/instance: loki
                app.kubernetes.io/name: loki
            topologyKey: kubernetes.io/hostname
      automountServiceAccountToken: true
      containers:
      - args:
        - -config.file=/etc/loki/config/config.yaml
        - -target=all
        image: docker.io/grafana/loki:2.8.2
        imagePullPolicy: IfNotPresent
        name: loki
        ports:
        - containerPort: 3100
          name: http-metrics
          protocol: TCP
        - containerPort: 9095
          name: grpc
          protocol: TCP
        - containerPort: 7946
          name: http-memberlist
          protocol: TCP
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 30
          timeoutSeconds: 1
        resources: {}
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
        volumeMounts:
        - mountPath: /tmp
          name: tmp
        - mountPath: /etc/loki/config
          name: config
        - mountPath: /etc/loki/runtime-config
          name: runtime-config
        - mountPath: /var/loki
          name: storage
      enableServiceLinks: true
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
      serviceAccountName: loki
      terminationGracePeriodSeconds: 30
      volumes:
      - emptyDir: {}
        name: tmp
      - configMap:
          name: loki
        name: config
      - configMap:
          name: loki-runtime
        name: runtime-config
  updateStrategy:
    rollingUpdate:
      partition: 0
  volumeClaimTemplates:
  - metadata:
      name: storage
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 10Gi
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: alertmanager
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: 2.8.0
    helm.sh/chart: mimir-distributed-4.4.1
  name: mimir-alertmanager
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: alertmanager
      app.kubernetes.io/instance: mimir
      app.kubernetes.io/name: mimir
  serviceName: mimir-alertmanager
  template:
    metadata:
      annotations:
        checksum/alertmanager-fallback-config: 86bde4d2ea5d9e520ca4b3ea79389e524380d014cb89dbe85cbdb52a968f4bd5
        checksum/config: d816162f2de5bc934e373c238e23069f055826ea2c378c29c0d1941356b87873
      labels:
        app.kubernetes.io/component: alertmanager
        app.kubernetes.io/instance: mimir
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: mimir
        app.kubernetes.io/part-of: memberlist
        app.kubernetes.io/version: 2.8.0
        helm.sh/chart: mimir-distributed-4.4.1
      namespace: monitoring
    spec:
      affinity: {}
      containers:
      - args:
        - -target=alertmanager
        - -config.expand-env=true
        - -config.file=/etc/mimir/mimir.yaml
        env: null
        envFrom: null
        image: grafana/mimir:2.8.0
        imagePullPolicy: IfNotPresent
        livenessProbe: null
        name: alertmanager
        ports:
        - containerPort: 8080
          name: http-metrics
          protocol: TCP
        - containerPort: 9095
          name: grpc
          protocol: TCP
        - containerPort: 7946
          name: memberlist
          protocol: TCP
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 45
        resources:
          requests:
            cpu: 10m
            memory: 32Mi
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
        volumeMounts:
        - mountPath: /etc/mimir
          name: config
        - mountPath: /var/mimir
          name: runtime-config
        - mountPath: /data
          name: storage
        - mountPath: /configs/
          name: alertmanager-fallback-config
        - mountPath: /tmp
          name: tmp
        - mountPath: /active-query-tracker
          name: active-queries
      initContainers: []
      nodeSelector: {}
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: mimir
      terminationGracePeriodSeconds: 60
      tolerations: []
      topologySpreadConstraints:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/component: alertmanager
            app.kubernetes.io/instance: mimir
            app.kubernetes.io/name: mimir
        maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
      volumes:
      - configMap:
          items:
          - key: mimir.yaml
            path: mimir.yaml
          name: mimir-config
        name: config
      - configMap:
          name: mimir-runtime
        name: runtime-config
      - emptyDir: {}
        name: tmp
      - emptyDir: {}
        name: active-queries
      - configMap:
          name: mimir-alertmanager-fallback-config
        name: alertmanager-fallback-config
  updateStrategy:
    type: RollingUpdate
  volumeClaimTemplates:
  - metadata:
      name: storage
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 1Gi
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: compactor
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: 2.8.0
    helm.sh/chart: mimir-distributed-4.4.1
  name: mimir-compactor
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: compactor
      app.kubernetes.io/instance: mimir
      app.kubernetes.io/name: mimir
  serviceName: mimir-compactor
  template:
    metadata:
      annotations:
        checksum/config: d816162f2de5bc934e373c238e23069f055826ea2c378c29c0d1941356b87873
      labels:
        app.kubernetes.io/component: compactor
        app.kubernetes.io/instance: mimir
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: mimir
        app.kubernetes.io/part-of: memberlist
        app.kubernetes.io/version: 2.8.0
        helm.sh/chart: mimir-distributed-4.4.1
      namespace: monitoring
    spec:
      affinity: {}
      containers:
      - args:
        - -target=compactor
        - -config.expand-env=true
        - -config.file=/etc/mimir/mimir.yaml
        env: null
        envFrom: null
        image: grafana/mimir:2.8.0
        imagePullPolicy: IfNotPresent
        livenessProbe: null
        name: compactor
        ports:
        - containerPort: 8080
          name: http-metrics
          protocol: TCP
        - containerPort: 9095
          name: grpc
          protocol: TCP
        - containerPort: 7946
          name: memberlist
          protocol: TCP
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 60
        resources:
          requests:
            cpu: 100m
            memory: 512Mi
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
        volumeMounts:
        - mountPath: /etc/mimir
          name: config
        - mountPath: /var/mimir
          name: runtime-config
        - mountPath: /data
          name: storage
        - mountPath: /active-query-tracker
          name: active-queries
      initContainers: []
      nodeSelector: {}
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: mimir
      terminationGracePeriodSeconds: 240
      tolerations: []
      topologySpreadConstraints:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/component: compactor
            app.kubernetes.io/instance: mimir
            app.kubernetes.io/name: mimir
        maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
      volumes:
      - configMap:
          items:
          - key: mimir.yaml
            path: mimir.yaml
          name: mimir-config
        name: config
      - configMap:
          name: mimir-runtime
        name: runtime-config
      - emptyDir: {}
        name: active-queries
  updateStrategy:
    type: RollingUpdate
  volumeClaimTemplates:
  - metadata:
      name: storage
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 2Gi
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: ingester
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: 2.8.0
    helm.sh/chart: mimir-distributed-4.4.1
  name: mimir-ingester
  namespace: monitoring
spec:
  podManagementPolicy: Parallel
  replicas: 3
  selector:
    matchLabels:
      app.kubernetes.io/component: ingester
      app.kubernetes.io/instance: mimir
      app.kubernetes.io/name: mimir
  serviceName: mimir-ingester-headless
  template:
    metadata:
      annotations:
        checksum/config: d816162f2de5bc934e373c238e23069f055826ea2c378c29c0d1941356b87873
      labels:
        app.kubernetes.io/component: ingester
        app.kubernetes.io/instance: mimir
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: mimir
        app.kubernetes.io/part-of: memberlist
        app.kubernetes.io/version: 2.8.0
        helm.sh/chart: mimir-distributed-4.4.1
      namespace: monitoring
    spec:
      affinity: {}
      containers:
      - args:
        - -target=ingester
        - -config.expand-env=true
        - -config.file=/etc/mimir/mimir.yaml
        - -ingester.ring.instance-availability-zone=zone-default
        env: null
        envFrom: null
        image: grafana/mimir:2.8.0
        imagePullPolicy: IfNotPresent
        livenessProbe: null
        name: ingester
        ports:
        - containerPort: 8080
          name: http-metrics
          protocol: TCP
        - containerPort: 9095
          name: grpc
          protocol: TCP
        - containerPort: 7946
          name: memberlist
          protocol: TCP
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 60
        resources:
          requests:
            cpu: 100m
            memory: 512Mi
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
        volumeMounts:
        - mountPath: /etc/mimir
          name: config
        - mountPath: /var/mimir
          name: runtime-config
        - mountPath: /data
          name: storage
        - mountPath: /active-query-tracker
          name: active-queries
      initContainers: []
      nodeSelector: {}
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: mimir
      terminationGracePeriodSeconds: 240
      tolerations: []
      topologySpreadConstraints:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/component: ingester
            app.kubernetes.io/instance: mimir
            app.kubernetes.io/name: mimir
        maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
      volumes:
      - configMap:
          items:
          - key: mimir.yaml
            path: mimir.yaml
          name: mimir-config
        name: config
      - configMap:
          name: mimir-runtime
        name: runtime-config
      - emptyDir: {}
        name: active-queries
  updateStrategy:
    type: RollingUpdate
  volumeClaimTemplates:
  - metadata:
      name: storage
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 100Gi
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: store-gateway
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: 2.8.0
    helm.sh/chart: mimir-distributed-4.4.1
  name: mimir-store-gateway
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: store-gateway
      app.kubernetes.io/instance: mimir
      app.kubernetes.io/name: mimir
  serviceName: mimir-store-gateway-headless
  template:
    metadata:
      annotations:
        checksum/config: d816162f2de5bc934e373c238e23069f055826ea2c378c29c0d1941356b87873
      labels:
        app.kubernetes.io/component: store-gateway
        app.kubernetes.io/instance: mimir
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: mimir
        app.kubernetes.io/part-of: memberlist
        app.kubernetes.io/version: 2.8.0
        helm.sh/chart: mimir-distributed-4.4.1
      namespace: monitoring
    spec:
      affinity: {}
      containers:
      - args:
        - -target=store-gateway
        - -config.expand-env=true
        - -config.file=/etc/mimir/mimir.yaml
        env: null
        envFrom: null
        image: grafana/mimir:2.8.0
        imagePullPolicy: IfNotPresent
        livenessProbe: null
        name: store-gateway
        ports:
        - containerPort: 8080
          name: http-metrics
          protocol: TCP
        - containerPort: 9095
          name: grpc
          protocol: TCP
        - containerPort: 7946
          name: memberlist
          protocol: TCP
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 60
        resources:
          requests:
            cpu: 100m
            memory: 512Mi
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
        volumeMounts:
        - mountPath: /etc/mimir
          name: config
        - mountPath: /var/mimir
          name: runtime-config
        - mountPath: /data
          name: storage
        - mountPath: /active-query-tracker
          name: active-queries
      initContainers: []
      nodeSelector: {}
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: mimir
      terminationGracePeriodSeconds: 240
      tolerations: []
      topologySpreadConstraints:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/component: store-gateway
            app.kubernetes.io/instance: mimir
            app.kubernetes.io/name: mimir
        maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
      volumes:
      - configMap:
          items:
          - key: mimir.yaml
            path: mimir.yaml
          name: mimir-config
        name: config
      - configMap:
          name: mimir-runtime
        name: runtime-config
      - emptyDir: {}
        name: active-queries
  updateStrategy:
    type: RollingUpdate
  volumeClaimTemplates:
  - metadata:
      name: storage
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 2Gi
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    app.kubernetes.io/component: ingester
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/version: 2.8.0
    helm.sh/chart: mimir-distributed-4.4.1
  name: mimir-ingester
  namespace: monitoring
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: ingester
      app.kubernetes.io/instance: mimir
      app.kubernetes.io/name: mimir
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    app.kubernetes.io/component: store-gateway
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/version: 2.8.0
    helm.sh/chart: mimir-distributed-4.4.1
  name: mimir-store-gateway
  namespace: monitoring
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: store-gateway
      app.kubernetes.io/instance: mimir
      app.kubernetes.io/name: mimir
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app.kubernetes.io/instance: grafana-agent
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana-agent
    app.kubernetes.io/version: v0.33.1
    helm.sh/chart: grafana-agent-0.13.0
  name: grafana-agent
  namespace: monitoring
spec:
  minReadySeconds: 10
  selector:
    matchLabels:
      app.kubernetes.io/instance: grafana-agent
      app.kubernetes.io/name: grafana-agent
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: grafana-agent
        app.kubernetes.io/name: grafana-agent
    spec:
      containers:
      - args:
        - run
        - /etc/agent/config.river
        - --storage.path=/tmp/agent
        - --server.http.listen-addr=0.0.0.0:80
        env:
        - name: AGENT_MODE
          value: flow
        - name: HOSTNAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        image: grafana/agent:v0.33.1
        imagePullPolicy: IfNotPresent
        name: grafana-agent
        ports:
        - containerPort: 80
          name: http-metrics
        - containerPort: 12347
          name: faro
        readinessProbe:
          httpGet:
            path: /-/ready
            port: 80
          initialDelaySeconds: 10
          timeoutSeconds: 1
        volumeMounts:
        - mountPath: /etc/agent
          name: config
        - mountPath: /var/log
          name: varlog
          readOnly: true
        - mountPath: /var/lib/docker/containers
          name: dockercontainers
          readOnly: true
        - mountPath: /host/root
          name: rootfs
        - mountPath: /host/sys
          name: sysfs
        - mountPath: /host/proc
          name: procfs
      - args:
        - --volume-dir=/etc/agent
        - --webhook-url=http://localhost:80/-/reload
        image: jimmidyson/configmap-reload:v0.8.0
        name: config-reloader
        volumeMounts:
        - mountPath: /etc/agent
          name: config
      dnsPolicy: ClusterFirst
      serviceAccount: grafana-agent
      volumes:
      - configMap:
          name: grafana-agent
        name: config
      - hostPath:
          path: /var/log
        name: varlog
      - hostPath:
          path: /var/lib/docker/containers
        name: dockercontainers
      - hostPath:
          path: /
        name: rootfs
      - hostPath:
          path: /sys
        name: sysfs
      - hostPath:
          path: /proc
        name: procfs
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app.kubernetes.io/component: canary
    app.kubernetes.io/instance: loki
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: loki
    app.kubernetes.io/version: 2.8.2
    helm.sh/chart: loki-5.5.0
  name: loki-canary
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app.kubernetes.io/component: canary
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
  template:
    metadata:
      annotations: null
      labels:
        app.kubernetes.io/component: canary
        app.kubernetes.io/instance: loki
        app.kubernetes.io/name: loki
    spec:
      containers:
      - args:
        - -addr=loki-gateway.monitoring.svc.cluster.local.
        - -labelname=pod
        - -labelvalue=$(POD_NAME)
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        image: docker.io/grafana/loki-canary:2.8.2
        imagePullPolicy: IfNotPresent
        name: loki-canary
        ports:
        - containerPort: 3500
          name: http-metrics
          protocol: TCP
        readinessProbe:
          httpGet:
            path: /metrics
            port: http-metrics
          initialDelaySeconds: 15
          timeoutSeconds: 1
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
      serviceAccountName: loki-canary
---
apiVersion: batch/v1
kind: Job
metadata:
  labels:
    app: mimir-distributed-make-bucket-job
    chart: mimir-distributed-4.4.1
    heritage: Helm
    release: mimir
  name: mimir-make-minio-buckets-5.0.7
  namespace: monitoring
spec:
  template:
    metadata:
      labels:
        app: mimir-distributed-job
        release: mimir
    spec:
      containers:
      - command:
        - /bin/sh
        - /config/initialize
        env:
        - name: MINIO_ENDPOINT
          value: mimir-minio
        - name: MINIO_PORT
          value: "9000"
        image: quay.io/minio/mc:RELEASE.2023-01-28T20-29-38Z
        imagePullPolicy: IfNotPresent
        name: minio-mc
        resources:
          requests:
            memory: 128Mi
        volumeMounts:
        - mountPath: /config
          name: minio-configuration
      restartPolicy: OnFailure
      volumes:
      - name: minio-configuration
        projected:
          sources:
          - configMap:
              name: mimir-minio
          - secret:
              name: mimir-minio
---
apiVersion: batch/v1
kind: Job
metadata:
  annotations:
    helm.sh/hook: post-install,post-upgrade
    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation
  labels:
    app: minio-post-job
    chart: minio-5.0.7
    heritage: Helm
    release: mimir
  name: mimir-minio-post-job
  namespace: monitoring
spec:
  template:
    metadata:
      labels:
        app: minio-job
        release: mimir
    spec:
      containers:
      - command:
        - /bin/sh
        - /config/initialize
        env:
        - name: MINIO_ENDPOINT
          value: mimir-minio
        - name: MINIO_PORT
          value: "9000"
        image: quay.io/minio/mc:RELEASE.2023-01-28T20-29-38Z
        imagePullPolicy: IfNotPresent
        name: minio-make-bucket
        resources:
          requests:
            memory: 128Mi
        volumeMounts:
        - mountPath: /config
          name: minio-configuration
      - command:
        - /bin/sh
        - /config/add-user
        env:
        - name: MINIO_ENDPOINT
          value: mimir-minio
        - name: MINIO_PORT
          value: "9000"
        image: quay.io/minio/mc:RELEASE.2023-01-28T20-29-38Z
        imagePullPolicy: IfNotPresent
        name: minio-make-user
        resources:
          requests:
            memory: 128Mi
        volumeMounts:
        - mountPath: /config
          name: minio-configuration
      restartPolicy: OnFailure
      volumes:
      - name: minio-configuration
        projected:
          sources:
          - configMap:
              name: mimir-minio
          - secret:
              name: mimir-minio
---
apiVersion: batch/v1
kind: Job
metadata:
  annotations:
    helm.sh/hook: test
  labels:
    app.kubernetes.io/component: smoke-test
    app.kubernetes.io/instance: mimir
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mimir
    app.kubernetes.io/version: 2.8.0
    helm.sh/chart: mimir-distributed-4.4.1
  name: mimir-smoke-test
  namespace: monitoring
spec:
  backoffLimit: 5
  completions: 1
  parallelism: 1
  selector: null
  template:
    metadata:
      labels:
        app.kubernetes.io/component: smoke-test
        app.kubernetes.io/instance: mimir
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: mimir
        app.kubernetes.io/version: 2.8.0
        helm.sh/chart: mimir-distributed-4.4.1
    spec:
      containers:
      - args:
        - -tests.smoke-test
        - -tests.write-endpoint=http://mimir-nginx.monitoring.svc:80
        - -tests.read-endpoint=http://mimir-nginx.monitoring.svc:80/prometheus
        - -tests.tenant-id=
        - -tests.write-read-series-test.num-series=1000
        - -tests.write-read-series-test.max-query-age=48h
        - -server.metrics-port=8080
        env: null
        envFrom: null
        image: grafana/mimir-continuous-test:2.8.0
        imagePullPolicy: IfNotPresent
        name: smoke-test
        volumeMounts: null
      initContainers: []
      restartPolicy: OnFailure
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: mimir
      volumes: null
---
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  creationTimestamp: null
  name: grafana
  namespace: monitoring
spec:
  encryptedData:
    admin-password: AgAtmsd5Ptrj4fjXAP3uI0fYCA7aL+nNkbstF+cZXJ2vZthowFUMT663B4Y9A/8ekJd8FPHQwQJl3sXjRuW0/SD99I6cP0gWTafEWJWYf24mSMJsmo65B/vCGAdoc5Ts5RZvj0CqWirCDvNU6LhJ0PgYVOH5rXbfAL0dYydbRkdAFL8ts211MjPXGZc70MDrYHYZYFDStqW3MHezJ2AQlEmV3a51GsbX4bkU3mArX+9vxSit8Lxf2o9NfUkQzLN7aVy6+y9njwGN+D09g/t/UhIG0TluSVy/ONwR9ajVZryGryD8LsYdbduJURQl+sRWmqM76vXiTRHRU1dBJFW8LNxKvj6GnI8NG/RET9an/osU1uwGOUQ5lw3PGA4DR19muLsxs0t4VRo9r9wBrK9Io3k/ZJkYC+UpKbzWhRijw4j5Wpcv+56iLmSjzxpsr+aXFK0YfQqV866i5yL7l2ORI76yIwRM9snJWDwYfsHystypEPwHa25lU/+BNFS45JQOBIRi17eaue7Ts3nRnyeYuqphB5tp8RBQ7bkVLafj4W4wQxKFl1uAqn9pbADNzPAnmpQYldEZGiNnXrxoHu3rHOPevRB4B5BeChmEKwzauDVUXvxIvT+q9qCWJscbh4XxFpoARQE8FQe7633XayQtTSZFpgqqxLA3rwjgzPjEuTuUYw9ela7TIcpXdDz895lLDiQN7tbJnIj0
    admin-user: AgCOEFPs2v/tPPVEuX6UwDwSPvcC+mHDOu+EKzyuvqZ73QDVb3PsiZviEVebfpP+Se8pZzPcdcrSbTn7zvWVkXqjU89saBhyEMWt5+3x/Iw0xWAc+MV2FTtdaUBtmaUI85MFal9Stp39Xla/2vQORqIEHthhk+ZCYdmK4K60/5H7xvb171iG/DcEKEZLGjZ7U7MpJUcuSIJkeo1xdz9MhQZ0LiQCZf6lbV1gvvGyGWBfketqCY7R9MeSh8wyY0+dQOs9vNirHwrvISnjo7jjK1QORMYVWsHMz5UWiMTgzSvKumtv1m5bmQo69FxNWM+/IwEvQeC3v/A9+dniD2ktwf4LZwztC03na/NB3KxKsnbjWk5C87oa2CxhbkyGiVGy4IY4r0zTjdOJlSuNsi/uZzZCeLxD/82GJeZHQAKsXdYiBbi8xsaeqCHeln8qG5dOxEHpbO/t8EdZZrH5nvbVFTBC77JUvvY6cLQbHuWpRoVxmu94bitU6ueNZ6P2/fnwmldcUBjgKEBoNyTpN0b9klLe5BNysHBlB2zvQO/bzjV7Y7APfMazRpNTIS4ccev40QdxKgqSoCdkbf641fVfyTk9lNnk3yOvRR53zOb5ZTVPOb2t9jMgW+CXvPpF628/fk0nDnsPJy6F9JMkGxieEKicANwNr1sLDTXBkeC7dWG92ekhvpUTQhtsFc84/9KoVUtOmDakL1R/
  template:
    metadata:
      creationTimestamp: null
      name: grafana
      namespace: monitoring
